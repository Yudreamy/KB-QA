loading embedding...
computing similarity...
loading data...
training...
Writing to /BIGDATA1/nsfc_dx_1/zoey/github/KB-QA-master\summary

step: 1 loss: 76.14101
step: 2 loss: 75.76981
step: 3 loss: 76.53686
step: 4 loss: 75.53599
step: 5 loss: 75.22231
step: 6 loss: 74.341324
step: 7 loss: 76.691025
step: 8 loss: 76.64664
step: 9 loss: 74.4077
step: 10 loss: 76.28488
step: 11 loss: 76.378815
step: 12 loss: 76.64166
step: 13 loss: 75.92376
step: 14 loss: 75.498055
step: 15 loss: 74.92866
step: 16 loss: 75.60276
step: 17 loss: 75.78326
step: 18 loss: 74.80165
step: 19 loss: 72.598595
step: 20 loss: 76.61775
step: 21 loss: 75.88768
step: 22 loss: 73.893555
step: 23 loss: 73.77719
step: 24 loss: 76.22461
step: 25 loss: 76.94658
step: 26 loss: 74.99884
step: 27 loss: 71.55201
step: 28 loss: 75.19864
step: 29 loss: 75.34872
step: 30 loss: 75.38437
step: 31 loss: 71.99286
step: 32 loss: 77.73729
step: 33 loss: 75.01759
step: 34 loss: 75.24037
step: 35 loss: 73.392685
step: 36 loss: 71.66857
step: 37 loss: 76.49321
step: 38 loss: 76.47916
step: 39 loss: 72.3723
step: 40 loss: 58.434532
step: 41 loss: 71.66345
step: 42 loss: 72.220604
step: 43 loss: 74.21242
step: 44 loss: 73.995995
step: 45 loss: 71.06554
step: 46 loss: 70.77664
step: 47 loss: 74.74603
step: 48 loss: 73.71907
step: 49 loss: 68.63908
step: 50 loss: 73.01056
evaluating..
evaluation acc:  0.31436978911230995
evaluation acc(train):  0.34268891069676155
step: 51 loss: 72.966995
step: 52 loss: 72.42612
step: 53 loss: 71.38963
step: 54 loss: 73.709015
step: 55 loss: 73.30928
step: 56 loss: 70.21569
step: 57 loss: 69.98722
step: 58 loss: 69.96777
step: 59 loss: 70.22436
step: 60 loss: 72.43845
step: 61 loss: 71.58899
step: 62 loss: 70.594925
step: 63 loss: 71.057465
step: 64 loss: 73.32915
step: 65 loss: 72.561935
step: 66 loss: 70.8229
step: 67 loss: 67.631386
step: 68 loss: 73.02474
step: 69 loss: 74.32793
step: 70 loss: 69.37713
step: 71 loss: 69.01747
step: 72 loss: 72.879395
step: 73 loss: 73.188255
step: 74 loss: 72.10632
step: 75 loss: 70.2434
step: 76 loss: 68.73363
step: 77 loss: 72.980316
step: 78 loss: 73.84926
step: 79 loss: 68.298485
step: 80 loss: 56.824917
step: 81 loss: 66.15662
step: 82 loss: 67.29115
step: 83 loss: 70.3725
step: 84 loss: 70.36789
step: 85 loss: 66.60881
step: 86 loss: 66.47518
step: 87 loss: 69.55559
step: 88 loss: 71.64924
step: 89 loss: 67.38087
step: 90 loss: 70.095276
step: 91 loss: 69.414406
step: 92 loss: 67.540726
step: 93 loss: 66.73242
step: 94 loss: 68.215805
step: 95 loss: 66.34201
step: 96 loss: 67.77396
step: 97 loss: 65.19087
step: 98 loss: 68.32533
step: 99 loss: 62.91395
step: 100 loss: 68.03781
evaluating..
evaluation acc:  0.3536047081902894
evaluation acc(train):  0.3997055937193327
step: 101 loss: 68.09288
step: 102 loss: 65.62114
step: 103 loss: 66.503265
step: 104 loss: 68.37378
step: 105 loss: 70.868515
step: 106 loss: 68.73033
step: 107 loss: 65.639175
step: 108 loss: 72.60364
step: 109 loss: 68.61861
step: 110 loss: 63.9981
step: 111 loss: 64.81641
step: 112 loss: 71.57538
step: 113 loss: 70.74912
step: 114 loss: 68.63048
step: 115 loss: 65.29098
step: 116 loss: 65.29335
step: 117 loss: 69.16923
step: 118 loss: 65.8285
step: 119 loss: 61.351196
step: 120 loss: 56.232716
step: 121 loss: 66.04012
step: 122 loss: 64.01923
step: 123 loss: 65.78024
step: 124 loss: 65.700806
step: 125 loss: 61.256554
step: 126 loss: 60.204956
step: 127 loss: 62.02119
step: 128 loss: 68.29697
step: 129 loss: 60.338226
step: 130 loss: 67.74237
step: 131 loss: 65.01986
step: 132 loss: 63.51017
step: 133 loss: 64.203354
step: 134 loss: 66.668655
step: 135 loss: 64.89924
step: 136 loss: 63.373924
step: 137 loss: 64.21955
step: 138 loss: 63.412956
step: 139 loss: 57.35801
step: 140 loss: 66.52194
step: 141 loss: 64.029366
step: 142 loss: 64.763885
step: 143 loss: 61.289505
step: 144 loss: 66.562996
step: 145 loss: 69.46358
step: 146 loss: 63.869675
step: 147 loss: 59.935795
step: 148 loss: 68.25931
step: 149 loss: 64.09217
step: 150 loss: 59.816414
evaluating..
evaluation acc:  0.37273173124080433
evaluation acc(train):  0.4368989205103042
step: 151 loss: 63.645912
step: 152 loss: 65.48332
step: 153 loss: 67.762886
step: 154 loss: 65.62283
step: 155 loss: 62.853905
step: 156 loss: 59.578938
step: 157 loss: 65.04164
step: 158 loss: 64.18503
step: 159 loss: 59.67926
step: 160 loss: 52.721592
step: 161 loss: 64.41037
step: 162 loss: 62.558075
step: 163 loss: 64.74045
step: 164 loss: 63.86213
step: 165 loss: 57.07564
step: 166 loss: 56.491043
step: 167 loss: 59.211884
step: 168 loss: 65.974976
step: 169 loss: 60.804165
step: 170 loss: 64.930786
step: 171 loss: 60.258507
step: 172 loss: 58.97892
step: 173 loss: 59.717216
step: 174 loss: 61.150124
step: 175 loss: 62.683273
step: 176 loss: 60.652336
step: 177 loss: 60.452393
step: 178 loss: 59.471756
step: 179 loss: 54.481686
step: 180 loss: 61.669365
step: 181 loss: 61.326607
step: 182 loss: 59.376568
step: 183 loss: 61.733963
step: 184 loss: 61.86557
step: 185 loss: 64.9604
step: 186 loss: 62.90577
step: 187 loss: 58.16684
step: 188 loss: 64.641205
step: 189 loss: 60.099884
step: 190 loss: 57.60094
step: 191 loss: 60.610764
step: 192 loss: 62.13285
step: 193 loss: 65.2927
step: 194 loss: 62.315067
step: 195 loss: 62.44948
step: 196 loss: 62.312828
step: 197 loss: 60.124496
step: 198 loss: 61.515675
step: 199 loss: 58.31362
step: 200 loss: 52.917675
evaluating..
evaluation acc:  0.3830308974987739
evaluation acc(train):  0.4566241413150147
step: 201 loss: 62.340225
step: 202 loss: 59.591934
step: 203 loss: 58.86792
step: 204 loss: 59.5475
step: 205 loss: 52.766388
step: 206 loss: 57.108658
step: 207 loss: 58.34384
step: 208 loss: 64.1058
step: 209 loss: 58.125717
step: 210 loss: 66.02724
step: 211 loss: 60.233578
step: 212 loss: 57.88645
step: 213 loss: 57.257584
step: 214 loss: 59.343277
step: 215 loss: 58.027275
step: 216 loss: 58.073196
step: 217 loss: 57.28055
step: 218 loss: 59.849453
step: 219 loss: 52.841743
step: 220 loss: 60.092636
step: 221 loss: 60.1679
step: 222 loss: 58.092636
step: 223 loss: 58.92828
step: 224 loss: 59.18381
step: 225 loss: 62.68686
step: 226 loss: 60.99537
step: 227 loss: 59.408768
step: 228 loss: 61.069397
step: 229 loss: 59.32578
step: 230 loss: 56.694324
step: 231 loss: 59.928085
step: 232 loss: 61.482788
step: 233 loss: 63.693707
step: 234 loss: 61.116997
step: 235 loss: 62.326965
step: 236 loss: 57.637413
step: 237 loss: 59.21074
step: 238 loss: 59.26422
step: 239 loss: 55.43495
step: 240 loss: 47.843105
step: 241 loss: 61.022526
step: 242 loss: 59.375324
step: 243 loss: 56.774628
step: 244 loss: 57.367065
step: 245 loss: 53.87745
step: 246 loss: 52.484795
step: 247 loss: 59.24685
step: 248 loss: 64.55617
step: 249 loss: 55.945496
step: 250 loss: 60.88392
evaluating..
evaluation acc:  0.3805787150564002
evaluation acc(train):  0.4831207065750736
step: 251 loss: 57.518982
step: 252 loss: 55.504723
step: 253 loss: 55.765278
step: 254 loss: 56.503365
step: 255 loss: 59.25615
step: 256 loss: 60.466717
step: 257 loss: 55.63573
step: 258 loss: 58.512985
step: 259 loss: 49.660553
step: 260 loss: 55.61786
step: 261 loss: 59.959885
step: 262 loss: 56.484844
step: 263 loss: 56.44657
step: 264 loss: 58.66328
step: 265 loss: 62.3127
step: 266 loss: 57.53865
step: 267 loss: 55.112213
step: 268 loss: 61.426575
step: 269 loss: 57.63694
step: 270 loss: 52.362144
step: 271 loss: 58.01632
step: 272 loss: 56.93178
step: 273 loss: 62.052532
step: 274 loss: 60.19181
step: 275 loss: 57.660194
step: 276 loss: 54.00627
step: 277 loss: 57.63456
step: 278 loss: 57.54236
step: 279 loss: 52.970367
step: 280 loss: 47.230484
step: 281 loss: 60.245083
step: 282 loss: 56.19979
step: 283 loss: 53.244995
step: 284 loss: 55.659966
step: 285 loss: 52.597916
step: 286 loss: 50.416977
step: 287 loss: 56.476147
step: 288 loss: 58.018917
step: 289 loss: 55.46793
step: 290 loss: 58.70171
step: 291 loss: 55.791336
step: 292 loss: 52.08744
step: 293 loss: 54.255028
step: 294 loss: 55.343628
step: 295 loss: 60.27366
step: 296 loss: 57.525154
step: 297 loss: 54.61714
step: 298 loss: 55.58318
step: 299 loss: 47.6164
step: 300 loss: 54.121376
evaluating..
evaluation acc:  0.37812653261402646
evaluation acc(train):  0.4977428851815505
step: 301 loss: 59.176018
step: 302 loss: 55.402466
step: 303 loss: 53.720425
step: 304 loss: 56.913986
step: 305 loss: 59.89375
step: 306 loss: 55.55764
step: 307 loss: 53.786255
step: 308 loss: 58.70513
step: 309 loss: 55.466125
step: 310 loss: 52.594086
step: 311 loss: 58.506966
step: 312 loss: 58.328064
step: 313 loss: 59.112526
step: 314 loss: 60.253468
step: 315 loss: 59.45105
step: 316 loss: 53.4338
step: 317 loss: 55.09027
step: 318 loss: 55.78166
step: 319 loss: 54.19699
step: 320 loss: 43.828903
step: 321 loss: 55.003036
step: 322 loss: 56.449337
step: 323 loss: 54.90114
step: 324 loss: 51.688892
step: 325 loss: 47.807217
step: 326 loss: 49.155163
step: 327 loss: 53.230724
step: 328 loss: 56.673492
step: 329 loss: 54.258892
step: 330 loss: 56.799206
step: 331 loss: 50.105766
step: 332 loss: 51.149567
step: 333 loss: 51.28512
step: 334 loss: 53.53273
step: 335 loss: 55.88029
step: 336 loss: 56.177433
step: 337 loss: 53.20579
step: 338 loss: 56.207375
step: 339 loss: 47.66103
step: 340 loss: 52.857346
step: 341 loss: 56.84388
step: 342 loss: 54.794876
step: 343 loss: 51.75965
step: 344 loss: 54.441544
step: 345 loss: 56.57139
step: 346 loss: 54.249844
step: 347 loss: 53.839867
step: 348 loss: 57.195724
step: 349 loss: 53.099735
step: 350 loss: 50.931812
evaluating..
evaluation acc:  0.3898970083374203
evaluation acc(train):  0.5190382728164867
step: 351 loss: 55.399025
step: 352 loss: 56.42811
step: 353 loss: 55.636322
step: 354 loss: 59.876186
step: 355 loss: 56.087234
step: 356 loss: 53.557487
step: 357 loss: 54.937313
step: 358 loss: 54.806667
step: 359 loss: 50.983788
step: 360 loss: 43.941414
step: 361 loss: 55.02803
step: 362 loss: 53.164253
step: 363 loss: 51.001003
step: 364 loss: 49.984226
step: 365 loss: 46.20221
step: 366 loss: 51.33184
step: 367 loss: 51.54065
step: 368 loss: 57.416916
step: 369 loss: 52.462284
step: 370 loss: 56.643272
step: 371 loss: 52.828293
step: 372 loss: 49.75367
step: 373 loss: 48.27794
step: 374 loss: 52.070534
step: 375 loss: 54.12031
step: 376 loss: 54.334846
step: 377 loss: 51.609665
step: 378 loss: 52.964226
step: 379 loss: 46.90499
step: 380 loss: 51.82299
step: 381 loss: 53.72249
step: 382 loss: 53.033676
step: 383 loss: 53.299114
step: 384 loss: 50.49455
step: 385 loss: 56.693695
step: 386 loss: 51.30123
step: 387 loss: 51.122276
step: 388 loss: 55.760437
step: 389 loss: 52.45181
step: 390 loss: 48.899075
step: 391 loss: 54.219265
step: 392 loss: 52.979813
step: 393 loss: 55.33651
step: 394 loss: 57.749855
step: 395 loss: 54.502373
step: 396 loss: 52.821095
step: 397 loss: 52.920925
step: 398 loss: 52.291985
step: 399 loss: 50.874847
step: 400 loss: 42.770676
evaluating..
evaluation acc:  0.3957822461991172
evaluation acc(train):  0.5308145240431796
step: 401 loss: 54.26637
step: 402 loss: 51.176064
step: 403 loss: 48.923267
step: 404 loss: 49.400024
step: 405 loss: 44.576912
step: 406 loss: 46.073334
step: 407 loss: 51.221916
step: 408 loss: 55.152405
step: 409 loss: 53.32281
step: 410 loss: 56.086395
step: 411 loss: 50.131805
step: 412 loss: 47.56379
step: 413 loss: 48.669228
step: 414 loss: 52.756626
step: 415 loss: 52.912262
step: 416 loss: 54.076256
step: 417 loss: 53.206562
step: 418 loss: 51.65491
step: 419 loss: 43.59487
step: 420 loss: 49.215714
step: 421 loss: 54.02075
step: 422 loss: 50.379852
step: 423 loss: 47.439102
step: 424 loss: 51.078598
step: 425 loss: 56.430668
step: 426 loss: 53.062565
step: 427 loss: 50.304146
step: 428 loss: 53.648834
step: 429 loss: 50.79396
step: 430 loss: 48.42041
step: 431 loss: 52.981285
step: 432 loss: 53.24691
step: 433 loss: 54.285175
step: 434 loss: 56.60311
step: 435 loss: 55.12677
step: 436 loss: 51.48718
step: 437 loss: 49.103386
step: 438 loss: 52.483414
step: 439 loss: 47.27703
step: 440 loss: 40.906506
step: 441 loss: 50.80133
step: 442 loss: 51.722324
step: 443 loss: 51.12033
step: 444 loss: 47.95533
step: 445 loss: 45.56449
step: 446 loss: 46.19997
step: 447 loss: 50.177887
step: 448 loss: 53.29706
step: 449 loss: 50.373024
step: 450 loss: 54.424767
evaluating..
evaluation acc:  0.3908778813143698
evaluation acc(train):  0.5448478900883219
step: 451 loss: 49.407806
step: 452 loss: 44.829742
step: 453 loss: 45.61228
step: 454 loss: 50.225704
step: 455 loss: 51.967697
step: 456 loss: 50.774105
step: 457 loss: 49.65232
step: 458 loss: 51.004677
step: 459 loss: 42.238224
step: 460 loss: 47.082573
step: 461 loss: 53.15355
step: 462 loss: 47.609947
step: 463 loss: 47.587616
step: 464 loss: 48.768227
step: 465 loss: 52.573196
step: 466 loss: 48.77353
step: 467 loss: 50.752728
step: 468 loss: 51.644012
step: 469 loss: 49.36614
step: 470 loss: 44.722324
step: 471 loss: 53.51725
step: 472 loss: 50.19032
step: 473 loss: 56.453236
step: 474 loss: 53.617653
step: 475 loss: 52.925606
step: 476 loss: 49.189945
step: 477 loss: 51.16726
step: 478 loss: 51.551697
step: 479 loss: 47.47114
step: 480 loss: 38.705894
step: 481 loss: 48.91268
step: 482 loss: 51.456306
step: 483 loss: 47.286674
step: 484 loss: 45.595448
step: 485 loss: 44.186203
step: 486 loss: 45.50536
step: 487 loss: 51.015175
step: 488 loss: 50.300835
step: 489 loss: 53.314938
step: 490 loss: 54.745346
step: 491 loss: 50.363514
step: 492 loss: 43.971954
step: 493 loss: 46.824783
step: 494 loss: 48.3161
step: 495 loss: 51.598385
step: 496 loss: 48.826473
step: 497 loss: 49.765404
step: 498 loss: 50.13125
step: 499 loss: 40.5487
step: 500 loss: 44.031403
evaluating..
evaluation acc:  0.39725355566454146
evaluation acc(train):  0.5631992149165849
step: 501 loss: 51.537903
step: 502 loss: 48.377026
step: 503 loss: 45.52433
step: 504 loss: 47.847313
step: 505 loss: 50.52616
step: 506 loss: 48.994095
step: 507 loss: 48.440235
step: 508 loss: 51.797306
step: 509 loss: 47.38608
step: 510 loss: 42.400517
step: 511 loss: 51.813805
step: 512 loss: 50.04852
step: 513 loss: 52.145542
step: 514 loss: 53.599888
step: 515 loss: 52.90894
step: 516 loss: 47.822807
step: 517 loss: 49.755913
step: 518 loss: 49.87463
step: 519 loss: 45.797966
step: 520 loss: 37.19366
step: 521 loss: 49.63205
step: 522 loss: 49.88228
step: 523 loss: 47.177788
step: 524 loss: 45.270687
step: 525 loss: 43.285217
step: 526 loss: 42.028664
step: 527 loss: 49.150497
step: 528 loss: 50.33986
step: 529 loss: 49.522873
step: 530 loss: 50.569614
step: 531 loss: 48.626774
step: 532 loss: 44.024883
step: 533 loss: 47.08252
step: 534 loss: 47.63173
step: 535 loss: 48.318996
step: 536 loss: 49.226738
step: 537 loss: 48.392067
step: 538 loss: 49.65883
step: 539 loss: 39.589478
step: 540 loss: 44.178024
step: 541 loss: 50.353317
step: 542 loss: 46.062084
step: 543 loss: 44.55057
step: 544 loss: 44.504517
step: 545 loss: 48.854797
step: 546 loss: 48.59877
step: 547 loss: 51.123558
step: 548 loss: 50.419167
step: 549 loss: 44.65508
step: 550 loss: 44.461662
evaluating..
evaluation acc:  0.40166748406081415
evaluation acc(train):  0.5744847890088322
step: 551 loss: 50.158283
step: 552 loss: 51.044212
step: 553 loss: 53.79609
step: 554 loss: 54.384377
step: 555 loss: 50.04683
step: 556 loss: 46.43299
step: 557 loss: 49.494984
step: 558 loss: 47.09157
step: 559 loss: 45.233597
step: 560 loss: 38.895195
step: 561 loss: 48.666874
step: 562 loss: 48.841473
step: 563 loss: 45.687317
step: 564 loss: 43.225323
step: 565 loss: 41.075573
step: 566 loss: 42.083347
step: 567 loss: 48.35428
step: 568 loss: 51.806244
step: 569 loss: 48.21303
step: 570 loss: 49.25195
step: 571 loss: 47.248016
step: 572 loss: 43.256233
step: 573 loss: 43.769455
step: 574 loss: 45.83705
step: 575 loss: 49.95236
step: 576 loss: 48.796146
step: 577 loss: 45.59227
step: 578 loss: 48.36863
step: 579 loss: 38.856274
step: 580 loss: 42.46286
step: 581 loss: 48.643974
step: 582 loss: 47.168068
step: 583 loss: 43.425526
step: 584 loss: 43.704315
step: 585 loss: 48.98806
step: 586 loss: 46.846996
step: 587 loss: 50.15387
step: 588 loss: 51.826286
step: 589 loss: 46.198547
step: 590 loss: 45.229992
step: 591 loss: 46.82377
step: 592 loss: 46.32646
step: 593 loss: 51.191162
step: 594 loss: 52.03298
step: 595 loss: 49.86881
step: 596 loss: 45.846718
step: 597 loss: 46.386353
step: 598 loss: 48.113197
step: 599 loss: 47.23703
step: 600 loss: 36.710373
evaluating..
evaluation acc:  0.40657184894556153
evaluation acc(train):  0.5847890088321884
step: 601 loss: 47.779106
step: 602 loss: 48.289425
step: 603 loss: 44.131203
step: 604 loss: 43.3431
step: 605 loss: 39.685
step: 606 loss: 39.125957
step: 607 loss: 48.57905
step: 608 loss: 48.297127
step: 609 loss: 48.761795
step: 610 loss: 48.715797
step: 611 loss: 46.1735
step: 612 loss: 44.50286
step: 613 loss: 44.530037
step: 614 loss: 45.82923
step: 615 loss: 47.7902
step: 616 loss: 48.364136
step: 617 loss: 43.660255
step: 618 loss: 46.841465
step: 619 loss: 36.256035
step: 620 loss: 42.79329
step: 621 loss: 46.058685
step: 622 loss: 43.49247
step: 623 loss: 41.777355
step: 624 loss: 44.189537
step: 625 loss: 48.303482
step: 626 loss: 46.56092
step: 627 loss: 48.119087
step: 628 loss: 48.44975
step: 629 loss: 46.144653
step: 630 loss: 44.553444
step: 631 loss: 47.31154
step: 632 loss: 45.817883
step: 633 loss: 50.36285
step: 634 loss: 51.903225
step: 635 loss: 48.607414
step: 636 loss: 45.750546
step: 637 loss: 44.36325
step: 638 loss: 46.50086
step: 639 loss: 43.180428
step: 640 loss: 34.623203
step: 641 loss: 45.052628
step: 642 loss: 46.387177
step: 643 loss: 45.326286
step: 644 loss: 43.673004
step: 645 loss: 39.785137
step: 646 loss: 39.324196
step: 647 loss: 46.002716
step: 648 loss: 47.38528
step: 649 loss: 47.287903
step: 650 loss: 48.4835
evaluating..
evaluation acc:  0.40362923001471307
evaluation acc(train):  0.5939156035328753
step: 651 loss: 47.087975
step: 652 loss: 41.175816
step: 653 loss: 42.878365
step: 654 loss: 45.712875
step: 655 loss: 44.854733
step: 656 loss: 45.830444
step: 657 loss: 45.392242
step: 658 loss: 44.280235
step: 659 loss: 35.044445
step: 660 loss: 40.551582
step: 661 loss: 45.808197
step: 662 loss: 44.54391
step: 663 loss: 40.527054
step: 664 loss: 43.16719
step: 665 loss: 46.29339
step: 666 loss: 41.20628
step: 667 loss: 45.91721
step: 668 loss: 47.5651
step: 669 loss: 44.989635
step: 670 loss: 42.005585
step: 671 loss: 46.818512
step: 672 loss: 46.997345
step: 673 loss: 47.304718
step: 674 loss: 50.591892
step: 675 loss: 44.689243
step: 676 loss: 42.576786
step: 677 loss: 45.124672
step: 678 loss: 43.730362
step: 679 loss: 42.505592
step: 680 loss: 36.311382
step: 681 loss: 43.388863
step: 682 loss: 47.052593
step: 683 loss: 46.339817
step: 684 loss: 40.736687
step: 685 loss: 39.49728
step: 686 loss: 38.512486
step: 687 loss: 46.946877
step: 688 loss: 47.972996
step: 689 loss: 45.835136
step: 690 loss: 43.76994
step: 691 loss: 43.966637
step: 692 loss: 40.627945
step: 693 loss: 42.58539
step: 694 loss: 42.22106
step: 695 loss: 48.708904
step: 696 loss: 45.18215
step: 697 loss: 45.755013
step: 698 loss: 44.39751
step: 699 loss: 34.921303
step: 700 loss: 37.818424
evaluating..
evaluation acc:  0.40951446787641
evaluation acc(train):  0.6135426889106967
step: 701 loss: 45.438587
step: 702 loss: 42.014812
step: 703 loss: 39.98526
step: 704 loss: 41.35066
step: 705 loss: 43.9659
step: 706 loss: 41.87365
step: 707 loss: 45.916172
step: 708 loss: 46.216396
step: 709 loss: 42.082672
step: 710 loss: 40.713623
step: 711 loss: 46.32805
step: 712 loss: 43.135006
step: 713 loss: 48.162018
step: 714 loss: 49.659454
step: 715 loss: 46.442875
step: 716 loss: 42.568447
step: 717 loss: 43.578125
step: 718 loss: 45.595226
step: 719 loss: 41.288044
step: 720 loss: 33.34766
step: 721 loss: 44.703907
step: 722 loss: 46.68078
step: 723 loss: 42.61236
step: 724 loss: 41.183704
step: 725 loss: 35.83213
step: 726 loss: 37.33194
step: 727 loss: 46.690403
step: 728 loss: 46.228638
step: 729 loss: 44.705055
step: 730 loss: 45.64305
step: 731 loss: 42.581886
step: 732 loss: 39.20307
step: 733 loss: 39.892456
step: 734 loss: 42.56571
step: 735 loss: 45.29223
step: 736 loss: 41.841656
step: 737 loss: 44.744892
step: 738 loss: 44.30735
step: 739 loss: 31.91716
step: 740 loss: 39.172436
step: 741 loss: 46.194168
step: 742 loss: 42.21938
step: 743 loss: 38.339043
step: 744 loss: 41.8298
step: 745 loss: 43.67912
step: 746 loss: 42.857197
step: 747 loss: 46.03809
step: 748 loss: 47.12018
step: 749 loss: 40.537773
step: 750 loss: 39.847404
evaluating..
evaluation acc:  0.40461010299166256
evaluation acc(train):  0.6252208047105005
step: 751 loss: 47.050663
step: 752 loss: 43.71424
step: 753 loss: 46.523636
step: 754 loss: 48.386074
step: 755 loss: 41.77955
step: 756 loss: 42.262024
step: 757 loss: 42.673885
step: 758 loss: 42.44624
step: 759 loss: 43.186356
step: 760 loss: 33.625114
step: 761 loss: 41.608025
step: 762 loss: 45.673817
step: 763 loss: 42.20346
step: 764 loss: 42.383602
step: 765 loss: 36.15944
step: 766 loss: 37.156513
step: 767 loss: 45.92193
step: 768 loss: 45.256775
step: 769 loss: 45.041855
step: 770 loss: 43.513412
step: 771 loss: 39.776867
step: 772 loss: 38.12111
step: 773 loss: 42.079918
step: 774 loss: 40.14174
step: 775 loss: 43.348953
step: 776 loss: 42.8571
step: 777 loss: 42.508057
step: 778 loss: 43.078728
step: 779 loss: 33.774467
step: 780 loss: 39.15841
step: 781 loss: 44.914574
step: 782 loss: 41.363167
step: 783 loss: 39.35311
step: 784 loss: 40.10102
step: 785 loss: 40.78015
step: 786 loss: 42.753284
step: 787 loss: 44.489067
step: 788 loss: 44.937294
step: 789 loss: 39.466263
step: 790 loss: 41.228603
step: 791 loss: 43.06949
step: 792 loss: 38.36483
step: 793 loss: 45.96604
step: 794 loss: 43.69425
step: 795 loss: 43.898033
step: 796 loss: 42.022133
step: 797 loss: 42.01177
step: 798 loss: 43.995872
step: 799 loss: 41.457764
step: 800 loss: 30.59714
evaluating..
evaluation acc:  0.41147621383030897
evaluation acc(train):  0.630323846908734
step: 801 loss: 39.653694
step: 802 loss: 45.690773
step: 803 loss: 40.02195
step: 804 loss: 37.759857
step: 805 loss: 36.606766
step: 806 loss: 35.4852
step: 807 loss: 42.579536
step: 808 loss: 43.339863
step: 809 loss: 44.028183
step: 810 loss: 42.58282
step: 811 loss: 41.15654
step: 812 loss: 37.625877
step: 813 loss: 39.005333
step: 814 loss: 39.825024
step: 815 loss: 43.32941
step: 816 loss: 40.89457
step: 817 loss: 42.470215
step: 818 loss: 40.489067
step: 819 loss: 30.863453
step: 820 loss: 37.678566
step: 821 loss: 45.3196
step: 822 loss: 38.79419
step: 823 loss: 37.084072
step: 824 loss: 39.961514
step: 825 loss: 41.970375
step: 826 loss: 40.566967
step: 827 loss: 41.345657
step: 828 loss: 42.698128
step: 829 loss: 38.915844
step: 830 loss: 36.528816
step: 831 loss: 41.37788
step: 832 loss: 38.455826
step: 833 loss: 43.86016
step: 834 loss: 45.01019
step: 835 loss: 40.596527
step: 836 loss: 37.555923
step: 837 loss: 43.24556
step: 838 loss: 41.570343
step: 839 loss: 37.644585
step: 840 loss: 29.42573
step: 841 loss: 39.441753
step: 842 loss: 42.32415
step: 843 loss: 41.091797
step: 844 loss: 36.33199
step: 845 loss: 35.91275
step: 846 loss: 35.41194
step: 847 loss: 43.881416
step: 848 loss: 41.365013
step: 849 loss: 43.955208
step: 850 loss: 45.55339
evaluating..
evaluation acc:  0.40902403138793525
evaluation acc(train):  0.639450441609421
step: 851 loss: 40.64103
step: 852 loss: 37.515
step: 853 loss: 37.39675
step: 854 loss: 39.129524
step: 855 loss: 43.29445
step: 856 loss: 41.82272
step: 857 loss: 41.19287
step: 858 loss: 40.946667
step: 859 loss: 30.723997
step: 860 loss: 34.245956
step: 861 loss: 47.01462
step: 862 loss: 39.83297
step: 863 loss: 33.839233
step: 864 loss: 39.339302
step: 865 loss: 39.40748
step: 866 loss: 39.135376
step: 867 loss: 43.492092
step: 868 loss: 41.758553
step: 869 loss: 37.89351
step: 870 loss: 38.383865
step: 871 loss: 43.11152
step: 872 loss: 39.55522
step: 873 loss: 44.167015
step: 874 loss: 45.067497
step: 875 loss: 42.35438
step: 876 loss: 36.303066
step: 877 loss: 39.40512
step: 878 loss: 40.492302
step: 879 loss: 36.40143
step: 880 loss: 29.34093
step: 881 loss: 40.24909
step: 882 loss: 41.09819
step: 883 loss: 37.741676
step: 884 loss: 37.926743
step: 885 loss: 34.53031
step: 886 loss: 34.307495
step: 887 loss: 43.6001
step: 888 loss: 38.84366
step: 889 loss: 42.034023
step: 890 loss: 42.93173
step: 891 loss: 40.924576
step: 892 loss: 34.8183
step: 893 loss: 37.839344
step: 894 loss: 42.772125
step: 895 loss: 42.439808
step: 896 loss: 42.51191
step: 897 loss: 38.990566
step: 898 loss: 39.788567
step: 899 loss: 27.701645
step: 900 loss: 33.99539
evaluating..
evaluation acc:  0.40804315841098576
evaluation acc(train):  0.6450441609421002
step: 901 loss: 43.348137
step: 902 loss: 39.669563
step: 903 loss: 34.492455
step: 904 loss: 38.34964
step: 905 loss: 41.386124
step: 906 loss: 38.546124
step: 907 loss: 40.372612
step: 908 loss: 42.25297
step: 909 loss: 36.69107
step: 910 loss: 35.585358
step: 911 loss: 40.56281
step: 912 loss: 38.73589
step: 913 loss: 44.36422
step: 914 loss: 46.2955
step: 915 loss: 41.46537
step: 916 loss: 36.950928
step: 917 loss: 39.332283
step: 918 loss: 43.28093
step: 919 loss: 35.235626
step: 920 loss: 28.946001
step: 921 loss: 39.4581
step: 922 loss: 44.625572
step: 923 loss: 38.038322
step: 924 loss: 33.06027
step: 925 loss: 34.1727
step: 926 loss: 34.767338
step: 927 loss: 41.32989
step: 928 loss: 39.078598
step: 929 loss: 41.371456
step: 930 loss: 40.210873
step: 931 loss: 37.909893
step: 932 loss: 34.889446
step: 933 loss: 39.865265
step: 934 loss: 39.289112
step: 935 loss: 39.635635
step: 936 loss: 40.38769
step: 937 loss: 38.71996
step: 938 loss: 40.465683
step: 939 loss: 28.197006
step: 940 loss: 34.57346
step: 941 loss: 42.752922
step: 942 loss: 37.24224
step: 943 loss: 37.16148
step: 944 loss: 37.26896
step: 945 loss: 40.212975
step: 946 loss: 37.007137
step: 947 loss: 41.827366
step: 948 loss: 41.09533
step: 949 loss: 35.537262
step: 950 loss: 34.55165
evaluating..
evaluation acc:  0.4139283962726827
evaluation acc(train):  0.6500490677134445
step: 951 loss: 39.349747
step: 952 loss: 37.380627
step: 953 loss: 41.278416
step: 954 loss: 43.05518
step: 955 loss: 39.429794
step: 956 loss: 37.835632
step: 957 loss: 39.058434
step: 958 loss: 39.38923
step: 959 loss: 35.26164
step: 960 loss: 27.782726
step: 961 loss: 40.01799
step: 962 loss: 39.98368
step: 963 loss: 37.66645
step: 964 loss: 35.532974
step: 965 loss: 34.393707
step: 966 loss: 33.44272
step: 967 loss: 42.123497
step: 968 loss: 38.77865
step: 969 loss: 43.035015
step: 970 loss: 40.964355
step: 971 loss: 35.43416
step: 972 loss: 35.557972
step: 973 loss: 36.94495
step: 974 loss: 39.22216
step: 975 loss: 40.545578
step: 976 loss: 40.08673
step: 977 loss: 40.350086
step: 978 loss: 39.16014
step: 979 loss: 28.135008
step: 980 loss: 34.68633
step: 981 loss: 41.798046
step: 982 loss: 38.11883
step: 983 loss: 34.54113
step: 984 loss: 36.352036
step: 985 loss: 40.175644
step: 986 loss: 37.081898
step: 987 loss: 38.672775
step: 988 loss: 42.63192
step: 989 loss: 37.910133
step: 990 loss: 33.364845
step: 991 loss: 41.564133
step: 992 loss: 38.885906
step: 993 loss: 42.449287
step: 994 loss: 41.833855
step: 995 loss: 37.874653
step: 996 loss: 36.469807
step: 997 loss: 38.68033
step: 998 loss: 37.085564
step: 999 loss: 37.393837
step: 1000 loss: 29.523602
evaluating..
evaluation acc:  0.4183423246689554
evaluation acc(train):  0.6565260058881256
step: 1001 loss: 36.086426
step: 1002 loss: 42.39216
step: 1003 loss: 37.1671
step: 1004 loss: 35.248314
step: 1005 loss: 33.73564
step: 1006 loss: 34.052967
step: 1007 loss: 40.99486
step: 1008 loss: 37.677822
step: 1009 loss: 42.77195
step: 1010 loss: 40.95957
step: 1011 loss: 40.041405
step: 1012 loss: 34.40777
step: 1013 loss: 33.744232
step: 1014 loss: 37.52275
step: 1015 loss: 41.728996
step: 1016 loss: 39.833557
step: 1017 loss: 37.433594
step: 1018 loss: 38.501835
step: 1019 loss: 27.73349
step: 1020 loss: 32.88048
step: 1021 loss: 42.014362
step: 1022 loss: 37.30786
step: 1023 loss: 36.322563
step: 1024 loss: 35.842342
step: 1025 loss: 39.751095
step: 1026 loss: 35.781685
step: 1027 loss: 39.81549
step: 1028 loss: 40.840874
step: 1029 loss: 39.220848
step: 1030 loss: 37.38439
step: 1031 loss: 39.288757
step: 1032 loss: 36.9503
step: 1033 loss: 42.319767
step: 1034 loss: 43.60292
step: 1035 loss: 38.353855
step: 1036 loss: 37.434452
step: 1037 loss: 38.55168
step: 1038 loss: 36.816414
step: 1039 loss: 36.14526
step: 1040 loss: 27.931593
step: 1041 loss: 36.378353
step: 1042 loss: 41.219143
step: 1043 loss: 34.425903
step: 1044 loss: 34.959564
step: 1045 loss: 33.368
step: 1046 loss: 32.3651
step: 1047 loss: 38.492744
step: 1048 loss: 38.42897
step: 1049 loss: 40.683052
step: 1050 loss: 38.72802
evaluating..
evaluation acc:  0.4217753800882786
evaluation acc(train):  0.6620215897939156
step: 1051 loss: 37.884323
step: 1052 loss: 31.109953
step: 1053 loss: 35.370644
step: 1054 loss: 38.612137
step: 1055 loss: 39.492115
step: 1056 loss: 41.26883
step: 1057 loss: 39.323895
step: 1058 loss: 41.401104
step: 1059 loss: 27.433277
step: 1060 loss: 31.253698
step: 1061 loss: 43.50773
step: 1062 loss: 38.36133
step: 1063 loss: 35.57008
step: 1064 loss: 35.173626
step: 1065 loss: 36.9008
step: 1066 loss: 35.4821
step: 1067 loss: 40.990643
step: 1068 loss: 41.456177
step: 1069 loss: 36.577286
step: 1070 loss: 34.411552
step: 1071 loss: 38.89473
step: 1072 loss: 36.725735
step: 1073 loss: 42.82406
step: 1074 loss: 41.654266
step: 1075 loss: 38.918896
step: 1076 loss: 35.496758
step: 1077 loss: 36.733635
step: 1078 loss: 40.26415
step: 1079 loss: 36.638065
step: 1080 loss: 28.826292
step: 1081 loss: 36.37351
step: 1082 loss: 39.723053
step: 1083 loss: 38.219746
step: 1084 loss: 34.49108
step: 1085 loss: 33.069344
step: 1086 loss: 33.706066
step: 1087 loss: 39.118073
step: 1088 loss: 37.642826
step: 1089 loss: 39.709507
step: 1090 loss: 39.30284
step: 1091 loss: 35.415688
step: 1092 loss: 34.17424
step: 1093 loss: 37.033684
step: 1094 loss: 36.90061
step: 1095 loss: 37.0213
step: 1096 loss: 39.550945
step: 1097 loss: 38.25129
step: 1098 loss: 39.66127
step: 1099 loss: 26.924196
step: 1100 loss: 32.152214
evaluating..
evaluation acc:  0.4198136341343796
evaluation acc(train):  0.6678115799803729
step: 1101 loss: 40.348824
step: 1102 loss: 35.599373
step: 1103 loss: 33.99164
step: 1104 loss: 34.901268
step: 1105 loss: 35.39254
step: 1106 loss: 36.586395
step: 1107 loss: 40.832977
step: 1108 loss: 39.89045
step: 1109 loss: 34.515026
step: 1110 loss: 34.050873
step: 1111 loss: 36.586937
step: 1112 loss: 34.028034
step: 1113 loss: 43.76855
step: 1114 loss: 41.059536
step: 1115 loss: 37.686295
step: 1116 loss: 35.775505
step: 1117 loss: 38.44057
step: 1118 loss: 37.772648
step: 1119 loss: 33.5892
step: 1120 loss: 29.099352
step: 1121 loss: 37.62432
step: 1122 loss: 40.345684
step: 1123 loss: 36.218395
step: 1124 loss: 34.811794
step: 1125 loss: 33.49699
step: 1126 loss: 32.50048
step: 1127 loss: 39.928894
step: 1128 loss: 39.261055
step: 1129 loss: 39.446075
step: 1130 loss: 37.463448
step: 1131 loss: 39.14415
step: 1132 loss: 35.444305
step: 1133 loss: 37.17272
step: 1134 loss: 35.741535
step: 1135 loss: 37.601562
step: 1136 loss: 39.444336
step: 1137 loss: 37.78109
step: 1138 loss: 37.003143
step: 1139 loss: 26.218624
step: 1140 loss: 29.485565
step: 1141 loss: 41.08097
step: 1142 loss: 35.837193
step: 1143 loss: 33.54038
step: 1144 loss: 31.666311
step: 1145 loss: 35.505512
step: 1146 loss: 36.529694
step: 1147 loss: 38.11859
step: 1148 loss: 39.28014
step: 1149 loss: 35.64228
step: 1150 loss: 34.207
evaluating..
evaluation acc:  0.4242275625306523
evaluation acc(train):  0.673405299313052
step: 1151 loss: 39.81921
step: 1152 loss: 36.581673
step: 1153 loss: 42.599224
step: 1154 loss: 40.670284
step: 1155 loss: 36.837498
step: 1156 loss: 35.931328
step: 1157 loss: 38.104645
step: 1158 loss: 39.611626
step: 1159 loss: 33.83072
step: 1160 loss: 27.435799
step: 1161 loss: 36.383644
step: 1162 loss: 41.160843
step: 1163 loss: 35.89202
step: 1164 loss: 32.656242
step: 1165 loss: 33.47368
step: 1166 loss: 31.609222
step: 1167 loss: 38.706238
step: 1168 loss: 38.046944
step: 1169 loss: 40.949837
step: 1170 loss: 37.10594
step: 1171 loss: 39.318993
step: 1172 loss: 32.41391
step: 1173 loss: 34.88173
step: 1174 loss: 36.844734
step: 1175 loss: 35.61029
step: 1176 loss: 37.419334
step: 1177 loss: 37.15757
step: 1178 loss: 35.755783
step: 1179 loss: 26.069485
step: 1180 loss: 30.857218
step: 1181 loss: 40.352425
step: 1182 loss: 37.474255
step: 1183 loss: 33.830833
step: 1184 loss: 33.424294
step: 1185 loss: 36.71114
step: 1186 loss: 35.668404
step: 1187 loss: 39.02653
step: 1188 loss: 38.17914
step: 1189 loss: 38.32776
step: 1190 loss: 32.158863
step: 1191 loss: 37.018566
step: 1192 loss: 36.532303
step: 1193 loss: 41.40141
step: 1194 loss: 39.357666
step: 1195 loss: 38.524513
step: 1196 loss: 35.46072
step: 1197 loss: 35.765846
step: 1198 loss: 36.845764
step: 1199 loss: 36.918167
step: 1200 loss: 27.893864
evaluating..
evaluation acc:  0.426679744973026
evaluation acc(train):  0.6759568204121688
step: 1201 loss: 36.21894
step: 1202 loss: 38.82164
step: 1203 loss: 35.961185
step: 1204 loss: 33.29973
step: 1205 loss: 30.806309
step: 1206 loss: 31.030783
step: 1207 loss: 39.643234
step: 1208 loss: 35.733444
step: 1209 loss: 37.863987
step: 1210 loss: 37.879894
step: 1211 loss: 35.799717
step: 1212 loss: 34.046227
step: 1213 loss: 35.249374
step: 1214 loss: 34.722878
step: 1215 loss: 36.84655
step: 1216 loss: 37.846222
step: 1217 loss: 35.29848
step: 1218 loss: 38.011955
step: 1219 loss: 25.816412
step: 1220 loss: 30.232199
step: 1221 loss: 40.78981
step: 1222 loss: 36.26175
step: 1223 loss: 33.805428
step: 1224 loss: 32.659187
step: 1225 loss: 37.1344
step: 1226 loss: 34.79763
step: 1227 loss: 39.395977
step: 1228 loss: 40.71608
step: 1229 loss: 33.624756
step: 1230 loss: 33.287548
step: 1231 loss: 38.309967
step: 1232 loss: 34.759655
step: 1233 loss: 42.423893
step: 1234 loss: 40.24225
step: 1235 loss: 36.550022
step: 1236 loss: 35.42739
step: 1237 loss: 35.849007
step: 1238 loss: 36.812386
step: 1239 loss: 33.73751
step: 1240 loss: 27.178938
step: 1241 loss: 35.61364
step: 1242 loss: 40.601757
step: 1243 loss: 34.737736
step: 1244 loss: 32.01612
step: 1245 loss: 32.721462
step: 1246 loss: 33.304497
step: 1247 loss: 37.78935
step: 1248 loss: 36.05629
step: 1249 loss: 38.33821
step: 1250 loss: 36.64241
evaluating..
evaluation acc:  0.4252084355076018
evaluation acc(train):  0.6820412168792934
step: 1251 loss: 35.414368
step: 1252 loss: 31.867388
step: 1253 loss: 34.404236
step: 1254 loss: 36.657127
step: 1255 loss: 38.209724
step: 1256 loss: 36.633434
step: 1257 loss: 37.744087
step: 1258 loss: 35.101772
step: 1259 loss: 28.024961
step: 1260 loss: 32.731926
step: 1261 loss: 39.553085
step: 1262 loss: 35.26713
step: 1263 loss: 32.41108
step: 1264 loss: 33.739834
step: 1265 loss: 37.588688
step: 1266 loss: 35.62213
step: 1267 loss: 37.478832
step: 1268 loss: 37.886948
step: 1269 loss: 35.442047
step: 1270 loss: 32.590343
step: 1271 loss: 37.895283
step: 1272 loss: 34.737938
step: 1273 loss: 38.09538
step: 1274 loss: 39.36771
step: 1275 loss: 34.242413
step: 1276 loss: 33.398136
step: 1277 loss: 34.574234
step: 1278 loss: 37.562073
step: 1279 loss: 34.958054
step: 1280 loss: 27.21082
step: 1281 loss: 34.758453
step: 1282 loss: 39.624325
step: 1283 loss: 33.174892
step: 1284 loss: 33.891357
step: 1285 loss: 33.00953
step: 1286 loss: 32.09237
step: 1287 loss: 37.870995
step: 1288 loss: 34.82344
step: 1289 loss: 39.631287
step: 1290 loss: 35.372402
step: 1291 loss: 36.58344
step: 1292 loss: 34.188828
step: 1293 loss: 32.799595
step: 1294 loss: 34.61785
step: 1295 loss: 34.141186
step: 1296 loss: 36.815132
step: 1297 loss: 35.767227
step: 1298 loss: 35.98484
step: 1299 loss: 25.126625
step: 1300 loss: 30.983204
evaluating..
evaluation acc:  0.4188327611574301
evaluation acc(train):  0.6845927379784102
step: 1301 loss: 39.45295
step: 1302 loss: 33.97084
step: 1303 loss: 33.1661
step: 1304 loss: 32.857502
step: 1305 loss: 37.38743
step: 1306 loss: 33.804092
step: 1307 loss: 37.848442
step: 1308 loss: 35.887093
step: 1309 loss: 33.217957
step: 1310 loss: 32.041466
step: 1311 loss: 39.379955
step: 1312 loss: 33.049484
step: 1313 loss: 39.039032
step: 1314 loss: 40.94808
step: 1315 loss: 35.80545
step: 1316 loss: 34.40778
step: 1317 loss: 35.78326
step: 1318 loss: 34.957066
step: 1319 loss: 33.988785
step: 1320 loss: 25.2183
step: 1321 loss: 34.67097
step: 1322 loss: 38.458668
step: 1323 loss: 33.957615
step: 1324 loss: 32.73827
step: 1325 loss: 29.347248
step: 1326 loss: 30.801645
step: 1327 loss: 37.978325
step: 1328 loss: 35.45534
step: 1329 loss: 37.887146
step: 1330 loss: 36.41134
step: 1331 loss: 37.04229
step: 1332 loss: 33.46463
step: 1333 loss: 32.271893
step: 1334 loss: 34.47685
step: 1335 loss: 37.79533
step: 1336 loss: 36.13166
step: 1337 loss: 34.857086
step: 1338 loss: 35.17267
step: 1339 loss: 24.713175
step: 1340 loss: 30.182322
step: 1341 loss: 37.7509
step: 1342 loss: 32.7052
step: 1343 loss: 30.780277
step: 1344 loss: 31.803827
step: 1345 loss: 37.039635
step: 1346 loss: 33.830353
step: 1347 loss: 37.32075
step: 1348 loss: 36.847443
step: 1349 loss: 35.529865
step: 1350 loss: 31.696548
evaluating..
evaluation acc:  0.42569887199607653
evaluation acc(train):  0.6924435721295388
step: 1351 loss: 37.37109
step: 1352 loss: 32.542084
step: 1353 loss: 39.467773
step: 1354 loss: 38.48426
step: 1355 loss: 34.455353
step: 1356 loss: 33.119446
step: 1357 loss: 34.104954
step: 1358 loss: 34.670326
step: 1359 loss: 32.06873
step: 1360 loss: 27.202595
step: 1361 loss: 35.128563
step: 1362 loss: 38.015182
step: 1363 loss: 32.262558
step: 1364 loss: 33.466778
step: 1365 loss: 32.097023
step: 1366 loss: 29.855743
step: 1367 loss: 38.007477
step: 1368 loss: 32.759674
step: 1369 loss: 39.582764
step: 1370 loss: 35.4638
step: 1371 loss: 35.856632
step: 1372 loss: 32.92936
step: 1373 loss: 31.339596
step: 1374 loss: 35.06569
step: 1375 loss: 35.59361
step: 1376 loss: 33.277645
step: 1377 loss: 33.162838
step: 1378 loss: 36.09459
step: 1379 loss: 25.952248
step: 1380 loss: 29.53528
step: 1381 loss: 34.976837
step: 1382 loss: 34.95704
step: 1383 loss: 33.21621
step: 1384 loss: 35.005238
step: 1385 loss: 34.990295
step: 1386 loss: 33.810165
step: 1387 loss: 36.48588
step: 1388 loss: 37.525337
step: 1389 loss: 33.79216
step: 1390 loss: 31.4494
step: 1391 loss: 34.98632
step: 1392 loss: 33.020424
step: 1393 loss: 41.703663
step: 1394 loss: 40.48071
step: 1395 loss: 37.368797
step: 1396 loss: 33.595554
step: 1397 loss: 35.702126
step: 1398 loss: 35.258156
step: 1399 loss: 33.31098
step: 1400 loss: 25.69308
evaluating..
evaluation acc:  0.42275625306522807
evaluation acc(train):  0.6950932286555447
step: 1401 loss: 34.68537
step: 1402 loss: 37.31161
step: 1403 loss: 32.860893
step: 1404 loss: 31.630455
step: 1405 loss: 29.23769
step: 1406 loss: 31.710289
step: 1407 loss: 34.12035
step: 1408 loss: 33.673107
step: 1409 loss: 38.911167
step: 1410 loss: 34.279285
step: 1411 loss: 36.61989
step: 1412 loss: 29.946072
step: 1413 loss: 32.786415
step: 1414 loss: 34.06555
step: 1415 loss: 34.60845
step: 1416 loss: 37.031197
step: 1417 loss: 34.17717
step: 1418 loss: 34.13695
step: 1419 loss: 23.544
step: 1420 loss: 29.55565
step: 1421 loss: 37.207764
step: 1422 loss: 34.653877
step: 1423 loss: 29.647417
step: 1424 loss: 32.0679
step: 1425 loss: 35.82724
step: 1426 loss: 32.985035
step: 1427 loss: 36.85147
step: 1428 loss: 37.72876
step: 1429 loss: 34.402115
step: 1430 loss: 32.0536
step: 1431 loss: 37.020096
step: 1432 loss: 32.43889
step: 1433 loss: 38.28305
step: 1434 loss: 39.86105
step: 1435 loss: 34.07184
step: 1436 loss: 34.719368
step: 1437 loss: 34.65244
step: 1438 loss: 33.59309
step: 1439 loss: 36.27976
step: 1440 loss: 25.903692
step: 1441 loss: 33.171387
step: 1442 loss: 36.99124
step: 1443 loss: 31.464378
step: 1444 loss: 31.304493
step: 1445 loss: 30.197918
step: 1446 loss: 30.874777
step: 1447 loss: 35.628666
step: 1448 loss: 33.87609
step: 1449 loss: 37.964516
step: 1450 loss: 34.82687
evaluating..
evaluation acc:  0.42373712604217756
evaluation acc(train):  0.7003925417075564
step: 1451 loss: 34.92671
step: 1452 loss: 29.583385
step: 1453 loss: 31.581125
step: 1454 loss: 33.253098
step: 1455 loss: 33.48133
step: 1456 loss: 30.473816
step: 1457 loss: 35.71759
step: 1458 loss: 35.60228
step: 1459 loss: 27.268835
step: 1460 loss: 29.61077
step: 1461 loss: 37.23826
step: 1462 loss: 32.69977
step: 1463 loss: 30.245062
step: 1464 loss: 31.72449
step: 1465 loss: 34.697605
step: 1466 loss: 32.155304
step: 1467 loss: 37.189472
step: 1468 loss: 36.038628
step: 1469 loss: 30.71803
step: 1470 loss: 31.05401
step: 1471 loss: 36.645157
step: 1472 loss: 31.00714
step: 1473 loss: 37.20778
step: 1474 loss: 37.532402
step: 1475 loss: 34.691376
step: 1476 loss: 34.23183
step: 1477 loss: 32.27468
step: 1478 loss: 32.6728
step: 1479 loss: 35.036255
step: 1480 loss: 24.987225
step: 1481 loss: 35.341484
step: 1482 loss: 37.09095
step: 1483 loss: 33.234184
step: 1484 loss: 30.43599
step: 1485 loss: 29.824398
step: 1486 loss: 28.922295
step: 1487 loss: 36.164406
step: 1488 loss: 32.70799
step: 1489 loss: 34.49831
step: 1490 loss: 35.84871
step: 1491 loss: 34.058517
step: 1492 loss: 28.461376
step: 1493 loss: 32.158875
step: 1494 loss: 31.986912
step: 1495 loss: 33.260666
step: 1496 loss: 34.158722
step: 1497 loss: 34.570133
step: 1498 loss: 35.741615
step: 1499 loss: 23.769693
step: 1500 loss: 31.032612
evaluating..
evaluation acc:  0.4291319274153997
evaluation acc(train):  0.7003925417075564
step: 1501 loss: 36.022194
step: 1502 loss: 32.799374
step: 1503 loss: 30.038265
step: 1504 loss: 31.280151
step: 1505 loss: 33.406616
step: 1506 loss: 30.191092
step: 1507 loss: 36.63642
step: 1508 loss: 37.514465
step: 1509 loss: 30.569115
step: 1510 loss: 30.970564
step: 1511 loss: 36.469635
step: 1512 loss: 32.765366
step: 1513 loss: 39.622055
step: 1514 loss: 38.245564
step: 1515 loss: 32.133812
step: 1516 loss: 31.176643
step: 1517 loss: 34.59558
step: 1518 loss: 36.278046
step: 1519 loss: 32.92038
step: 1520 loss: 25.553152
step: 1521 loss: 31.475857
step: 1522 loss: 36.97244
step: 1523 loss: 32.538227
step: 1524 loss: 31.253212
step: 1525 loss: 31.060747
step: 1526 loss: 30.704758
step: 1527 loss: 35.738976
step: 1528 loss: 32.164085
step: 1529 loss: 36.40014
step: 1530 loss: 34.09167
step: 1531 loss: 37.58134
step: 1532 loss: 32.498222
step: 1533 loss: 33.059216
step: 1534 loss: 34.99137
step: 1535 loss: 32.45986
step: 1536 loss: 31.44609
step: 1537 loss: 34.141563
step: 1538 loss: 35.91319
step: 1539 loss: 21.24218
step: 1540 loss: 28.950922
step: 1541 loss: 37.52572
step: 1542 loss: 32.291595
step: 1543 loss: 32.646194
step: 1544 loss: 29.812801
step: 1545 loss: 33.602356
step: 1546 loss: 30.925592
step: 1547 loss: 32.992672
step: 1548 loss: 37.02162
step: 1549 loss: 31.44884
step: 1550 loss: 30.278034
evaluating..
evaluation acc:  0.4271701814615007
evaluation acc(train):  0.7120706575073602
step: 1551 loss: 37.420322
step: 1552 loss: 30.610346
step: 1553 loss: 39.21615
step: 1554 loss: 37.529682
step: 1555 loss: 32.02581
step: 1556 loss: 29.414879
step: 1557 loss: 33.547447
step: 1558 loss: 35.997814
step: 1559 loss: 31.538792
step: 1560 loss: 26.289673
step: 1561 loss: 33.29843
step: 1562 loss: 36.5754
step: 1563 loss: 31.167765
step: 1564 loss: 32.483074
step: 1565 loss: 28.376762
step: 1566 loss: 28.549915
step: 1567 loss: 35.45661
step: 1568 loss: 31.560276
step: 1569 loss: 35.720177
step: 1570 loss: 33.406612
step: 1571 loss: 34.01245
step: 1572 loss: 32.142803
step: 1573 loss: 32.65834
step: 1574 loss: 31.719402
step: 1575 loss: 34.508705
step: 1576 loss: 34.170013
step: 1577 loss: 31.767944
step: 1578 loss: 33.8074
step: 1579 loss: 22.790012
step: 1580 loss: 29.181149
step: 1581 loss: 36.937805
step: 1582 loss: 33.910927
step: 1583 loss: 30.436886
step: 1584 loss: 27.93145
step: 1585 loss: 33.040176
step: 1586 loss: 29.423004
step: 1587 loss: 35.970444
step: 1588 loss: 37.467445
step: 1589 loss: 33.74357
step: 1590 loss: 29.667923
step: 1591 loss: 35.437412
step: 1592 loss: 32.180523
step: 1593 loss: 37.29023
step: 1594 loss: 35.988148
step: 1595 loss: 32.31017
step: 1596 loss: 32.069534
step: 1597 loss: 33.16514
step: 1598 loss: 35.768497
step: 1599 loss: 33.064087
step: 1600 loss: 23.89909
evaluating..
evaluation acc:  0.4306032368808239
evaluation acc(train):  0.7144259077526988
step: 1601 loss: 32.348053
step: 1602 loss: 36.221176
step: 1603 loss: 31.88284
step: 1604 loss: 31.2891
step: 1605 loss: 27.272224
step: 1606 loss: 26.577385
step: 1607 loss: 33.81904
step: 1608 loss: 31.913061
step: 1609 loss: 34.25489
step: 1610 loss: 32.68182
step: 1611 loss: 34.514713
step: 1612 loss: 29.396503
step: 1613 loss: 32.059464
step: 1614 loss: 32.381588
step: 1615 loss: 32.149696
step: 1616 loss: 32.850296
step: 1617 loss: 33.619785
step: 1618 loss: 33.654648
step: 1619 loss: 22.553982
step: 1620 loss: 27.73884
step: 1621 loss: 34.51818
step: 1622 loss: 31.542515
step: 1623 loss: 28.292746
step: 1624 loss: 30.695803
step: 1625 loss: 32.411335
step: 1626 loss: 31.379257
step: 1627 loss: 35.84759
step: 1628 loss: 34.503677
step: 1629 loss: 29.901848
step: 1630 loss: 28.225094
step: 1631 loss: 34.452343
step: 1632 loss: 30.991314
step: 1633 loss: 36.9425
step: 1634 loss: 37.131794
step: 1635 loss: 31.513996
step: 1636 loss: 29.384388
step: 1637 loss: 31.59698
step: 1638 loss: 33.1201
step: 1639 loss: 30.580273
step: 1640 loss: 27.77238
step: 1641 loss: 31.4344
step: 1642 loss: 32.6356
step: 1643 loss: 31.569826
step: 1644 loss: 30.443916
step: 1645 loss: 28.73647
step: 1646 loss: 27.9997
step: 1647 loss: 35.167366
step: 1648 loss: 28.891945
step: 1649 loss: 34.09304
step: 1650 loss: 31.747478
evaluating..
evaluation acc:  0.43207454634624815
evaluation acc(train):  0.7153091265947007
step: 1651 loss: 32.538925
step: 1652 loss: 31.484016
step: 1653 loss: 32.198257
step: 1654 loss: 32.756454
step: 1655 loss: 32.90668
step: 1656 loss: 33.04954
step: 1657 loss: 32.896545
step: 1658 loss: 30.739199
step: 1659 loss: 23.566559
step: 1660 loss: 27.92381
step: 1661 loss: 34.343845
step: 1662 loss: 30.825867
step: 1663 loss: 28.114433
step: 1664 loss: 29.333946
step: 1665 loss: 31.392717
step: 1666 loss: 31.207743
step: 1667 loss: 31.893822
step: 1668 loss: 36.701927
step: 1669 loss: 30.254393
step: 1670 loss: 27.907797
step: 1671 loss: 33.542557
step: 1672 loss: 30.169106
step: 1673 loss: 37.432228
step: 1674 loss: 36.019875
step: 1675 loss: 30.72248
step: 1676 loss: 28.995361
step: 1677 loss: 30.518818
step: 1678 loss: 34.255817
step: 1679 loss: 32.831482
step: 1680 loss: 23.807814
step: 1681 loss: 31.509209
step: 1682 loss: 35.44909
step: 1683 loss: 31.125149
step: 1684 loss: 31.794067
step: 1685 loss: 28.584072
step: 1686 loss: 29.140863
step: 1687 loss: 30.937954
step: 1688 loss: 33.577682
step: 1689 loss: 34.71069
step: 1690 loss: 33.415703
step: 1691 loss: 34.2958
step: 1692 loss: 28.596123
step: 1693 loss: 31.271574
step: 1694 loss: 32.313946
step: 1695 loss: 32.260967
step: 1696 loss: 35.218605
step: 1697 loss: 31.978199
step: 1698 loss: 32.021095
step: 1699 loss: 21.291111
step: 1700 loss: 27.981735
evaluating..
evaluation acc:  0.42766061794997545
evaluation acc(train):  0.7158979391560353
step: 1701 loss: 37.832745
step: 1702 loss: 32.857452
step: 1703 loss: 30.827618
step: 1704 loss: 28.12831
step: 1705 loss: 32.503384
step: 1706 loss: 31.802572
step: 1707 loss: 35.192368
step: 1708 loss: 36.487137
step: 1709 loss: 30.579075
step: 1710 loss: 29.221428
step: 1711 loss: 31.918211
step: 1712 loss: 29.840694
step: 1713 loss: 34.75712
step: 1714 loss: 36.07351
step: 1715 loss: 30.66586
step: 1716 loss: 28.31213
step: 1717 loss: 30.770607
step: 1718 loss: 35.054497
step: 1719 loss: 30.891014
step: 1720 loss: 24.258028
step: 1721 loss: 29.995152
step: 1722 loss: 35.483475
step: 1723 loss: 32.00388
step: 1724 loss: 29.363913
step: 1725 loss: 29.11795
step: 1726 loss: 29.780355
step: 1727 loss: 34.47515
step: 1728 loss: 32.237553
step: 1729 loss: 34.092873
step: 1730 loss: 33.26867
step: 1731 loss: 33.06395
step: 1732 loss: 28.280867
step: 1733 loss: 31.27841
step: 1734 loss: 31.189415
step: 1735 loss: 32.327168
step: 1736 loss: 31.178698
step: 1737 loss: 31.511465
step: 1738 loss: 33.51858
step: 1739 loss: 21.631992
step: 1740 loss: 26.349115
step: 1741 loss: 35.696854
step: 1742 loss: 31.665684
step: 1743 loss: 28.381878
step: 1744 loss: 29.248035
step: 1745 loss: 32.30162
step: 1746 loss: 28.754675
step: 1747 loss: 32.36937
step: 1748 loss: 32.39435
step: 1749 loss: 28.667389
step: 1750 loss: 25.822315
evaluating..
evaluation acc:  0.4281510544384502
evaluation acc(train):  0.7194308145240432
step: 1751 loss: 33.36718
step: 1752 loss: 30.718943
step: 1753 loss: 35.9016
step: 1754 loss: 38.160427
step: 1755 loss: 31.251846
step: 1756 loss: 29.280975
step: 1757 loss: 31.20661
step: 1758 loss: 33.700348
step: 1759 loss: 30.62093
step: 1760 loss: 24.0743
step: 1761 loss: 30.551947
step: 1762 loss: 34.615005
step: 1763 loss: 31.37363
step: 1764 loss: 28.028147
step: 1765 loss: 25.558979
step: 1766 loss: 28.622252
step: 1767 loss: 31.784882
step: 1768 loss: 30.45582
step: 1769 loss: 31.871088
step: 1770 loss: 30.623943
step: 1771 loss: 34.21573
step: 1772 loss: 30.246975
step: 1773 loss: 28.805351
step: 1774 loss: 29.723341
step: 1775 loss: 30.354042
step: 1776 loss: 29.405178
step: 1777 loss: 33.330444
step: 1778 loss: 33.742493
step: 1779 loss: 22.820112
step: 1780 loss: 27.189363
step: 1781 loss: 35.10498
step: 1782 loss: 31.724846
step: 1783 loss: 29.472265
step: 1784 loss: 27.861757
step: 1785 loss: 31.665855
step: 1786 loss: 28.653019
step: 1787 loss: 33.17334
step: 1788 loss: 33.474
step: 1789 loss: 29.57727
step: 1790 loss: 29.755058
step: 1791 loss: 32.419456
step: 1792 loss: 30.881763
step: 1793 loss: 36.486603
step: 1794 loss: 35.87064
step: 1795 loss: 31.447939
step: 1796 loss: 28.894424
step: 1797 loss: 32.92188
step: 1798 loss: 33.578094
step: 1799 loss: 31.295582
step: 1800 loss: 24.925375
evaluating..
evaluation acc:  0.43109367336929866
evaluation acc(train):  0.7219823356231599
step: 1801 loss: 30.331444
step: 1802 loss: 34.68341
step: 1803 loss: 28.874859
step: 1804 loss: 29.72489
step: 1805 loss: 28.463408
step: 1806 loss: 28.316717
step: 1807 loss: 32.76844
step: 1808 loss: 31.159653
step: 1809 loss: 34.951824
step: 1810 loss: 31.851364
step: 1811 loss: 32.805378
step: 1812 loss: 29.404078
step: 1813 loss: 28.804302
step: 1814 loss: 29.805597
step: 1815 loss: 31.721365
step: 1816 loss: 31.632637
step: 1817 loss: 33.072407
step: 1818 loss: 32.332623
step: 1819 loss: 23.067139
step: 1820 loss: 26.390583
step: 1821 loss: 34.52839
step: 1822 loss: 30.308372
step: 1823 loss: 28.163233
step: 1824 loss: 27.633526
step: 1825 loss: 30.112906
step: 1826 loss: 27.797709
step: 1827 loss: 32.84831
step: 1828 loss: 33.382477
step: 1829 loss: 30.258629
step: 1830 loss: 28.65083
step: 1831 loss: 34.851738
step: 1832 loss: 29.020382
step: 1833 loss: 35.266624
step: 1834 loss: 34.535015
step: 1835 loss: 30.531857
step: 1836 loss: 27.61796
step: 1837 loss: 30.994982
step: 1838 loss: 33.97545
step: 1839 loss: 31.681894
step: 1840 loss: 21.999289
step: 1841 loss: 30.490612
step: 1842 loss: 34.813583
step: 1843 loss: 29.270592
step: 1844 loss: 29.018953
step: 1845 loss: 27.201014
step: 1846 loss: 28.185469
step: 1847 loss: 35.407417
step: 1848 loss: 31.938873
step: 1849 loss: 32.67028
step: 1850 loss: 31.050533
evaluating..
evaluation acc:  0.4340362923001471
evaluation acc(train):  0.7221786064769382
step: 1851 loss: 31.791233
step: 1852 loss: 29.155106
step: 1853 loss: 27.579771
step: 1854 loss: 32.9935
step: 1855 loss: 33.321762
step: 1856 loss: 29.402676
step: 1857 loss: 32.7835
step: 1858 loss: 32.543816
step: 1859 loss: 23.453133
step: 1860 loss: 27.353725
step: 1861 loss: 33.51663
step: 1862 loss: 29.797424
step: 1863 loss: 29.087902
step: 1864 loss: 31.550718
step: 1865 loss: 29.96627
step: 1866 loss: 28.376534
step: 1867 loss: 33.905376
step: 1868 loss: 33.2454
step: 1869 loss: 27.044296
step: 1870 loss: 27.506374
step: 1871 loss: 33.055176
step: 1872 loss: 29.004519
step: 1873 loss: 36.037556
step: 1874 loss: 35.325058
step: 1875 loss: 30.852402
step: 1876 loss: 27.064701
step: 1877 loss: 33.850857
step: 1878 loss: 33.110756
step: 1879 loss: 31.588444
step: 1880 loss: 23.497772
step: 1881 loss: 30.18307
step: 1882 loss: 32.664295
step: 1883 loss: 29.16443
step: 1884 loss: 30.99606
step: 1885 loss: 27.348751
step: 1886 loss: 30.204361
step: 1887 loss: 31.695663
step: 1888 loss: 31.705763
step: 1889 loss: 32.94333
step: 1890 loss: 30.802952
step: 1891 loss: 31.1185
step: 1892 loss: 29.45593
step: 1893 loss: 27.833996
step: 1894 loss: 32.625614
step: 1895 loss: 31.505922
step: 1896 loss: 30.613754
step: 1897 loss: 30.08032
step: 1898 loss: 30.263561
step: 1899 loss: 21.10754
step: 1900 loss: 27.402266
evaluating..
evaluation acc:  0.4315841098577734
evaluation acc(train):  0.7255152109911678
step: 1901 loss: 34.26709
step: 1902 loss: 28.695454
step: 1903 loss: 28.403671
step: 1904 loss: 27.998901
step: 1905 loss: 30.079374
step: 1906 loss: 28.71164
step: 1907 loss: 35.455482
step: 1908 loss: 32.34887
step: 1909 loss: 31.865871
step: 1910 loss: 28.525978
step: 1911 loss: 33.141254
step: 1912 loss: 31.132496
step: 1913 loss: 36.792786
step: 1914 loss: 34.973854
step: 1915 loss: 30.6877
step: 1916 loss: 27.359592
step: 1917 loss: 29.763262
step: 1918 loss: 32.36048
step: 1919 loss: 30.184011
step: 1920 loss: 24.649624
step: 1921 loss: 30.61699
step: 1922 loss: 33.838203
step: 1923 loss: 30.190699
step: 1924 loss: 28.709389
step: 1925 loss: 26.918293
step: 1926 loss: 27.80577
step: 1927 loss: 33.31377
step: 1928 loss: 29.102158
step: 1929 loss: 33.24768
step: 1930 loss: 34.378647
step: 1931 loss: 30.852089
step: 1932 loss: 30.015297
step: 1933 loss: 28.380108
step: 1934 loss: 31.853241
step: 1935 loss: 30.731567
step: 1936 loss: 31.3922
step: 1937 loss: 32.044884
step: 1938 loss: 29.874977
step: 1939 loss: 20.232613
step: 1940 loss: 25.771568
step: 1941 loss: 35.96791
step: 1942 loss: 30.127007
step: 1943 loss: 31.109354
step: 1944 loss: 28.898193
step: 1945 loss: 30.853895
step: 1946 loss: 27.88505
step: 1947 loss: 32.731163
step: 1948 loss: 33.17606
step: 1949 loss: 30.82863
step: 1950 loss: 27.747202
evaluating..
evaluation acc:  0.43795978420794507
evaluation acc(train):  0.7282630029440628
step: 1951 loss: 31.622482
step: 1952 loss: 27.083748
step: 1953 loss: 33.54338
step: 1954 loss: 34.02112
step: 1955 loss: 30.784414
step: 1956 loss: 28.466377
step: 1957 loss: 31.598711
step: 1958 loss: 34.066628
step: 1959 loss: 31.875952
step: 1960 loss: 22.77371
step: 1961 loss: 30.688385
step: 1962 loss: 31.19634
step: 1963 loss: 28.29874
step: 1964 loss: 29.570255
step: 1965 loss: 26.437145
step: 1966 loss: 28.89795
step: 1967 loss: 31.563158
step: 1968 loss: 28.44887
step: 1969 loss: 32.849438
step: 1970 loss: 31.845278
step: 1971 loss: 30.41141
step: 1972 loss: 28.400883
step: 1973 loss: 30.34205
step: 1974 loss: 31.829247
step: 1975 loss: 31.20435
step: 1976 loss: 32.06221
step: 1977 loss: 31.429249
step: 1978 loss: 30.568434
step: 1979 loss: 20.494513
step: 1980 loss: 26.163633
step: 1981 loss: 32.7676
step: 1982 loss: 28.504261
step: 1983 loss: 27.70895
step: 1984 loss: 28.47873
step: 1985 loss: 31.60067
step: 1986 loss: 28.358685
step: 1987 loss: 32.70717
step: 1988 loss: 33.28566
step: 1989 loss: 28.64281
step: 1990 loss: 28.844002
step: 1991 loss: 32.877464
step: 1992 loss: 28.228971
step: 1993 loss: 35.336533
step: 1994 loss: 35.638542
step: 1995 loss: 29.531929
step: 1996 loss: 29.011799
step: 1997 loss: 28.849133
step: 1998 loss: 32.23695
step: 1999 loss: 30.551685
step: 2000 loss: 24.170542
evaluating..
evaluation acc:  0.44090240313879353
evaluation acc(train):  0.7334641805691855
step: 2001 loss: 28.651672
step: 2002 loss: 32.51052
step: 2003 loss: 28.89037
step: 2004 loss: 27.273058
step: 2005 loss: 27.057858
step: 2006 loss: 26.670494
step: 2007 loss: 32.597145
step: 2008 loss: 27.74692
step: 2009 loss: 31.22127
step: 2010 loss: 30.481644
step: 2011 loss: 32.363396
step: 2012 loss: 28.849579
step: 2013 loss: 29.856348
step: 2014 loss: 31.61876
step: 2015 loss: 33.616974
step: 2016 loss: 31.882122
step: 2017 loss: 31.538803
step: 2018 loss: 30.338696
step: 2019 loss: 20.701118
step: 2020 loss: 26.121986
step: 2021 loss: 33.30244
step: 2022 loss: 31.198034
step: 2023 loss: 26.798033
step: 2024 loss: 29.49727
step: 2025 loss: 30.600946
step: 2026 loss: 29.067533
step: 2027 loss: 35.743706
step: 2028 loss: 33.683723
step: 2029 loss: 30.292778
step: 2030 loss: 28.343174
step: 2031 loss: 31.005249
step: 2032 loss: 29.305353
step: 2033 loss: 35.24396
step: 2034 loss: 35.82309
step: 2035 loss: 28.111595
step: 2036 loss: 27.757847
step: 2037 loss: 31.980259
step: 2038 loss: 31.395166
step: 2039 loss: 31.066603
step: 2040 loss: 24.945286
step: 2041 loss: 29.25232
step: 2042 loss: 33.237488
step: 2043 loss: 27.879822
step: 2044 loss: 28.119028
step: 2045 loss: 26.759045
step: 2046 loss: 27.906216
step: 2047 loss: 33.191124
step: 2048 loss: 28.00011
step: 2049 loss: 33.028122
step: 2050 loss: 30.24364
evaluating..
evaluation acc:  0.43550760176557135
evaluation acc(train):  0.7327772325809617
step: 2051 loss: 31.524017
step: 2052 loss: 27.69683
step: 2053 loss: 26.691023
step: 2054 loss: 28.570312
step: 2055 loss: 30.718784
step: 2056 loss: 31.261242
step: 2057 loss: 28.919287
step: 2058 loss: 31.697536
step: 2059 loss: 22.108313
step: 2060 loss: 23.105911
step: 2061 loss: 35.718746
step: 2062 loss: 29.583622
step: 2063 loss: 27.107296
step: 2064 loss: 28.202179
step: 2065 loss: 32.355656
step: 2066 loss: 28.153831
step: 2067 loss: 33.693237
step: 2068 loss: 30.098253
step: 2069 loss: 28.805876
step: 2070 loss: 27.53003
step: 2071 loss: 32.99936
step: 2072 loss: 29.420692
step: 2073 loss: 35.605698
step: 2074 loss: 34.565533
step: 2075 loss: 31.006012
step: 2076 loss: 26.621994
step: 2077 loss: 29.079231
step: 2078 loss: 31.940128
step: 2079 loss: 29.483913
step: 2080 loss: 23.468658
step: 2081 loss: 28.138676
step: 2082 loss: 33.97563
step: 2083 loss: 28.65177
step: 2084 loss: 27.303576
step: 2085 loss: 25.695929
step: 2086 loss: 28.014711
step: 2087 loss: 33.391224
step: 2088 loss: 28.562206
step: 2089 loss: 33.518173
step: 2090 loss: 27.954653
step: 2091 loss: 30.06906
step: 2092 loss: 28.810072
step: 2093 loss: 27.172836
step: 2094 loss: 30.656826
step: 2095 loss: 29.59487
step: 2096 loss: 30.914478
step: 2097 loss: 30.17463
step: 2098 loss: 31.17133
step: 2099 loss: 21.655993
step: 2100 loss: 24.356894
evaluating..
evaluation acc:  0.43550760176557135
evaluation acc(train):  0.7358194308145241
step: 2101 loss: 36.196404
step: 2102 loss: 29.502361
step: 2103 loss: 25.754673
step: 2104 loss: 28.142513
step: 2105 loss: 30.058346
step: 2106 loss: 29.529484
step: 2107 loss: 32.70039
step: 2108 loss: 32.445057
step: 2109 loss: 28.957144
step: 2110 loss: 26.55062
step: 2111 loss: 32.51734
step: 2112 loss: 27.563051
step: 2113 loss: 34.439312
step: 2114 loss: 34.425995
step: 2115 loss: 31.449627
step: 2116 loss: 27.857471
step: 2117 loss: 30.318232
step: 2118 loss: 33.332954
step: 2119 loss: 30.709324
step: 2120 loss: 22.2632
step: 2121 loss: 29.369894
step: 2122 loss: 33.89332
step: 2123 loss: 27.214172
step: 2124 loss: 31.100035
step: 2125 loss: 27.728588
step: 2126 loss: 27.972933
step: 2127 loss: 34.16028
step: 2128 loss: 30.016775
step: 2129 loss: 35.08022
step: 2130 loss: 29.696217
step: 2131 loss: 31.438923
step: 2132 loss: 28.678753
step: 2133 loss: 28.916767
step: 2134 loss: 30.100842
step: 2135 loss: 29.391577
step: 2136 loss: 31.645208
step: 2137 loss: 30.775826
step: 2138 loss: 30.962067
step: 2139 loss: 22.069527
step: 2140 loss: 24.077122
step: 2141 loss: 35.772217
step: 2142 loss: 29.523157
step: 2143 loss: 28.27842
step: 2144 loss: 27.295378
step: 2145 loss: 28.965185
step: 2146 loss: 28.873756
step: 2147 loss: 32.361897
step: 2148 loss: 30.937965
step: 2149 loss: 29.420494
step: 2150 loss: 26.744766
evaluating..
evaluation acc:  0.43992153016184404
evaluation acc(train):  0.7389597644749755
step: 2151 loss: 30.780222
step: 2152 loss: 27.835255
step: 2153 loss: 32.25756
step: 2154 loss: 35.748466
step: 2155 loss: 28.707615
step: 2156 loss: 26.331747
step: 2157 loss: 29.582996
step: 2158 loss: 32.04511
step: 2159 loss: 28.990456
step: 2160 loss: 22.332333
step: 2161 loss: 30.60757
step: 2162 loss: 31.49317
step: 2163 loss: 28.19106
step: 2164 loss: 28.401344
step: 2165 loss: 27.074871
step: 2166 loss: 26.624655
step: 2167 loss: 32.02314
step: 2168 loss: 29.852545
step: 2169 loss: 31.859663
step: 2170 loss: 28.701546
step: 2171 loss: 31.846184
step: 2172 loss: 28.466179
step: 2173 loss: 28.052446
step: 2174 loss: 30.822376
step: 2175 loss: 30.240166
step: 2176 loss: 31.006752
step: 2177 loss: 28.281109
step: 2178 loss: 29.091997
step: 2179 loss: 19.800177
step: 2180 loss: 26.036247
step: 2181 loss: 33.92392
step: 2182 loss: 30.018919
step: 2183 loss: 26.363743
step: 2184 loss: 27.579496
step: 2185 loss: 31.037945
step: 2186 loss: 28.091887
step: 2187 loss: 32.53281
step: 2188 loss: 31.134228
step: 2189 loss: 29.697317
step: 2190 loss: 27.001291
step: 2191 loss: 32.88652
step: 2192 loss: 29.599861
step: 2193 loss: 34.335888
step: 2194 loss: 33.24971
step: 2195 loss: 27.23688
step: 2196 loss: 29.034563
step: 2197 loss: 31.410572
step: 2198 loss: 33.079433
step: 2199 loss: 31.952585
step: 2200 loss: 22.744389
evaluating..
evaluation acc:  0.441883276115743
evaluation acc(train):  0.7400392541707557
step: 2201 loss: 28.046707
step: 2202 loss: 32.917473
step: 2203 loss: 27.936905
step: 2204 loss: 28.261683
step: 2205 loss: 25.357262
step: 2206 loss: 27.046658
step: 2207 loss: 31.09074
step: 2208 loss: 26.809034
step: 2209 loss: 34.32955
step: 2210 loss: 30.463858
step: 2211 loss: 32.121857
step: 2212 loss: 27.046967
step: 2213 loss: 27.842451
step: 2214 loss: 31.035715
step: 2215 loss: 33.03613
step: 2216 loss: 29.88969
step: 2217 loss: 30.0972
step: 2218 loss: 32.092957
step: 2219 loss: 20.54052
step: 2220 loss: 25.05059
step: 2221 loss: 31.163849
step: 2222 loss: 28.225195
step: 2223 loss: 25.737839
step: 2224 loss: 26.919561
step: 2225 loss: 31.718336
step: 2226 loss: 27.981932
step: 2227 loss: 31.648167
step: 2228 loss: 32.28629
step: 2229 loss: 28.156265
step: 2230 loss: 26.810362
step: 2231 loss: 30.591465
step: 2232 loss: 27.187296
step: 2233 loss: 33.896538
step: 2234 loss: 34.858097
step: 2235 loss: 28.654968
step: 2236 loss: 27.320044
step: 2237 loss: 30.517534
step: 2238 loss: 30.72919
step: 2239 loss: 29.366137
step: 2240 loss: 23.120522
step: 2241 loss: 29.680342
step: 2242 loss: 31.924019
step: 2243 loss: 29.437508
step: 2244 loss: 26.501068
step: 2245 loss: 26.349968
step: 2246 loss: 26.72548
step: 2247 loss: 32.22373
step: 2248 loss: 26.487026
step: 2249 loss: 32.381992
step: 2250 loss: 29.163946
evaluating..
evaluation acc:  0.4350171652770966
evaluation acc(train):  0.7370951913640824
step: 2251 loss: 34.799255
step: 2252 loss: 28.185127
step: 2253 loss: 29.15893
step: 2254 loss: 28.209454
step: 2255 loss: 29.13984
step: 2256 loss: 28.510956
step: 2257 loss: 29.47966
step: 2258 loss: 30.236317
step: 2259 loss: 19.794954
step: 2260 loss: 26.50938
step: 2261 loss: 34.32073
step: 2262 loss: 29.336582
step: 2263 loss: 29.208387
step: 2264 loss: 26.791506
step: 2265 loss: 30.701275
step: 2266 loss: 26.970417
step: 2267 loss: 35.127743
step: 2268 loss: 31.560343
step: 2269 loss: 27.978146
step: 2270 loss: 27.615545
step: 2271 loss: 32.57319
step: 2272 loss: 29.069336
step: 2273 loss: 36.506874
step: 2274 loss: 33.62277
step: 2275 loss: 28.33786
step: 2276 loss: 27.183262
step: 2277 loss: 27.832294
step: 2278 loss: 32.04248
step: 2279 loss: 30.035172
step: 2280 loss: 21.883743
step: 2281 loss: 29.14418
step: 2282 loss: 33.99656
step: 2283 loss: 27.591045
step: 2284 loss: 28.960152
step: 2285 loss: 26.59392
step: 2286 loss: 24.779087
step: 2287 loss: 32.71818
step: 2288 loss: 27.99484
step: 2289 loss: 31.375015
step: 2290 loss: 28.301682
step: 2291 loss: 31.455357
step: 2292 loss: 27.48206
step: 2293 loss: 27.792936
step: 2294 loss: 29.675411
step: 2295 loss: 29.842598
step: 2296 loss: 29.780756
step: 2297 loss: 29.888487
step: 2298 loss: 31.720959
step: 2299 loss: 21.24852
step: 2300 loss: 24.591122
evaluating..
evaluation acc:  0.4350171652770966
evaluation acc(train):  0.7400392541707557
step: 2301 loss: 32.055805
step: 2302 loss: 30.395401
step: 2303 loss: 26.12026
step: 2304 loss: 28.442455
step: 2305 loss: 30.150106
step: 2306 loss: 28.438732
step: 2307 loss: 31.41905
step: 2308 loss: 32.079605
step: 2309 loss: 27.930248
step: 2310 loss: 26.97193
step: 2311 loss: 31.686853
step: 2312 loss: 29.265182
step: 2313 loss: 33.83726
step: 2314 loss: 32.905243
step: 2315 loss: 27.321323
step: 2316 loss: 26.58868
step: 2317 loss: 30.224236
step: 2318 loss: 31.763798
step: 2319 loss: 30.457775
step: 2320 loss: 22.047812
step: 2321 loss: 26.803509
step: 2322 loss: 32.526108
step: 2323 loss: 27.094955
step: 2324 loss: 27.199778
step: 2325 loss: 25.917141
step: 2326 loss: 24.219252
step: 2327 loss: 31.838192
step: 2328 loss: 27.997686
step: 2329 loss: 29.841312
step: 2330 loss: 29.059504
step: 2331 loss: 30.439537
step: 2332 loss: 27.031342
step: 2333 loss: 28.409088
step: 2334 loss: 28.823072
step: 2335 loss: 28.681608
step: 2336 loss: 28.217022
step: 2337 loss: 30.490314
step: 2338 loss: 29.758995
step: 2339 loss: 21.058887
step: 2340 loss: 23.488224
step: 2341 loss: 32.01566
step: 2342 loss: 29.407915
step: 2343 loss: 26.952545
step: 2344 loss: 27.843607
step: 2345 loss: 30.126965
step: 2346 loss: 26.997454
step: 2347 loss: 31.781502
step: 2348 loss: 32.579437
step: 2349 loss: 28.282177
step: 2350 loss: 25.695711
evaluating..
evaluation acc:  0.43305541932319763
evaluation acc(train):  0.7428851815505397
step: 2351 loss: 31.005054
step: 2352 loss: 26.902012
step: 2353 loss: 33.609566
step: 2354 loss: 33.24372
step: 2355 loss: 25.879618
step: 2356 loss: 26.696436
step: 2357 loss: 29.666666
step: 2358 loss: 32.02605
step: 2359 loss: 30.662682
step: 2360 loss: 22.801876
step: 2361 loss: 27.800196
step: 2362 loss: 31.64907
step: 2363 loss: 27.16616
step: 2364 loss: 27.015055
step: 2365 loss: 26.939774
step: 2366 loss: 24.385725
step: 2367 loss: 31.02748
step: 2368 loss: 27.208603
step: 2369 loss: 33.89424
step: 2370 loss: 29.038017
step: 2371 loss: 31.386703
step: 2372 loss: 27.07148
step: 2373 loss: 27.444622
step: 2374 loss: 29.140621
step: 2375 loss: 28.809464
step: 2376 loss: 28.734093
step: 2377 loss: 29.952263
step: 2378 loss: 30.774836
step: 2379 loss: 20.830502
step: 2380 loss: 24.35448
step: 2381 loss: 32.637043
step: 2382 loss: 30.35902
step: 2383 loss: 26.59552
step: 2384 loss: 28.505688
step: 2385 loss: 26.944304
step: 2386 loss: 26.775131
step: 2387 loss: 30.95575
step: 2388 loss: 33.638153
step: 2389 loss: 29.829407
step: 2390 loss: 25.444183
step: 2391 loss: 31.078808
step: 2392 loss: 26.88324
step: 2393 loss: 32.97961
step: 2394 loss: 32.82824
step: 2395 loss: 26.399479
step: 2396 loss: 24.577404
step: 2397 loss: 30.611166
step: 2398 loss: 34.422474
step: 2399 loss: 27.899586
step: 2400 loss: 22.27552
evaluating..
evaluation acc:  0.4404119666503188
evaluation acc(train):  0.7479882237487733
step: 2401 loss: 25.693031
step: 2402 loss: 30.889206
step: 2403 loss: 28.54052
step: 2404 loss: 27.779577
step: 2405 loss: 25.212849
step: 2406 loss: 25.910324
step: 2407 loss: 30.842026
step: 2408 loss: 28.021832
step: 2409 loss: 29.91112
step: 2410 loss: 27.644325
step: 2411 loss: 30.811092
step: 2412 loss: 27.742828
step: 2413 loss: 28.357874
step: 2414 loss: 29.485012
step: 2415 loss: 27.467209
step: 2416 loss: 27.826677
step: 2417 loss: 28.4504
step: 2418 loss: 30.187138
step: 2419 loss: 17.941969
step: 2420 loss: 22.815723
step: 2421 loss: 31.289564
step: 2422 loss: 29.999489
step: 2423 loss: 26.93996
step: 2424 loss: 27.63625
step: 2425 loss: 29.9457
step: 2426 loss: 27.616158
step: 2427 loss: 33.871895
step: 2428 loss: 31.55912
step: 2429 loss: 28.132133
step: 2430 loss: 26.73523
step: 2431 loss: 31.929523
step: 2432 loss: 28.261631
step: 2433 loss: 32.109528
step: 2434 loss: 34.099907
step: 2435 loss: 27.93827
step: 2436 loss: 26.981936
step: 2437 loss: 29.026733
step: 2438 loss: 30.996414
step: 2439 loss: 28.793333
step: 2440 loss: 20.609112
step: 2441 loss: 29.361046
step: 2442 loss: 32.11718
step: 2443 loss: 27.155338
step: 2444 loss: 28.842249
step: 2445 loss: 24.380478
step: 2446 loss: 23.7709
step: 2447 loss: 32.06268
step: 2448 loss: 28.2185
step: 2449 loss: 32.006504
step: 2450 loss: 27.597898
evaluating..
evaluation acc:  0.43648847474252084
evaluation acc(train):  0.7474975466143278
step: 2451 loss: 31.956203
step: 2452 loss: 25.842783
step: 2453 loss: 25.990417
step: 2454 loss: 26.837084
step: 2455 loss: 29.321844
step: 2456 loss: 28.45625
step: 2457 loss: 28.046726
step: 2458 loss: 29.424051
step: 2459 loss: 20.875158
step: 2460 loss: 22.044453
step: 2461 loss: 33.191864
step: 2462 loss: 28.71103
step: 2463 loss: 26.855518
step: 2464 loss: 23.718485
step: 2465 loss: 28.33704
step: 2466 loss: 27.502369
step: 2467 loss: 32.175957
step: 2468 loss: 30.206398
step: 2469 loss: 27.606575
step: 2470 loss: 23.492586
step: 2471 loss: 33.82353
step: 2472 loss: 26.350618
step: 2473 loss: 32.650284
step: 2474 loss: 33.703735
step: 2475 loss: 28.9066
step: 2476 loss: 24.554565
step: 2477 loss: 29.018208
step: 2478 loss: 30.408892
step: 2479 loss: 28.96328
step: 2480 loss: 19.65156
step: 2481 loss: 29.141926
step: 2482 loss: 32.028435
step: 2483 loss: 27.819149
step: 2484 loss: 26.05761
step: 2485 loss: 24.655579
step: 2486 loss: 26.681877
step: 2487 loss: 29.137539
step: 2488 loss: 25.649376
step: 2489 loss: 30.7642
step: 2490 loss: 29.612965
step: 2491 loss: 31.403797
step: 2492 loss: 26.29233
step: 2493 loss: 26.789959
step: 2494 loss: 28.189089
step: 2495 loss: 30.01082
step: 2496 loss: 28.2242
step: 2497 loss: 27.33472
step: 2498 loss: 29.096096
step: 2499 loss: 17.793894
step: 2500 loss: 24.008312
evaluating..
evaluation acc:  0.43894065718489456
evaluation acc(train):  0.7484789008832189
step: 2501 loss: 33.446854
step: 2502 loss: 27.889473
step: 2503 loss: 25.4094
step: 2504 loss: 25.395687
step: 2505 loss: 29.524168
step: 2506 loss: 27.24931
step: 2507 loss: 31.413956
step: 2508 loss: 32.41543
step: 2509 loss: 27.06745
step: 2510 loss: 27.547497
step: 2511 loss: 30.769943
step: 2512 loss: 27.65747
step: 2513 loss: 35.12596
step: 2514 loss: 35.32525
step: 2515 loss: 27.087011
step: 2516 loss: 24.791897
step: 2517 loss: 29.227053
step: 2518 loss: 31.703554
step: 2519 loss: 28.175617
step: 2520 loss: 22.647947
step: 2521 loss: 27.102154
step: 2522 loss: 30.66044
step: 2523 loss: 26.853409
step: 2524 loss: 27.235321
step: 2525 loss: 25.225803
step: 2526 loss: 25.267986
step: 2527 loss: 31.678862
step: 2528 loss: 27.999771
step: 2529 loss: 29.449385
step: 2530 loss: 26.35899
step: 2531 loss: 31.823517
step: 2532 loss: 26.735378
step: 2533 loss: 29.318848
step: 2534 loss: 28.749924
step: 2535 loss: 29.536667
step: 2536 loss: 27.754135
step: 2537 loss: 27.586731
step: 2538 loss: 29.599102
step: 2539 loss: 18.367142
step: 2540 loss: 24.66407
step: 2541 loss: 31.120352
step: 2542 loss: 30.385214
step: 2543 loss: 25.413227
step: 2544 loss: 26.963375
step: 2545 loss: 27.311039
step: 2546 loss: 25.054363
step: 2547 loss: 29.867622
step: 2548 loss: 28.917019
step: 2549 loss: 27.403755
step: 2550 loss: 28.123892
evaluating..
evaluation acc:  0.4394310936733693
evaluation acc(train):  0.7475956820412168
step: 2551 loss: 30.663548
step: 2552 loss: 26.633087
step: 2553 loss: 34.34842
step: 2554 loss: 33.579277
step: 2555 loss: 26.98441
step: 2556 loss: 26.02812
step: 2557 loss: 28.587852
step: 2558 loss: 31.57333
step: 2559 loss: 28.36814
step: 2560 loss: 20.070816
step: 2561 loss: 28.408497
step: 2562 loss: 31.259514
step: 2563 loss: 25.212107
step: 2564 loss: 26.059212
step: 2565 loss: 23.31756
step: 2566 loss: 24.466034
step: 2567 loss: 28.461353
step: 2568 loss: 26.291939
step: 2569 loss: 33.56093
step: 2570 loss: 26.684128
step: 2571 loss: 29.92759
step: 2572 loss: 26.68303
step: 2573 loss: 26.603813
step: 2574 loss: 29.607893
step: 2575 loss: 27.923723
step: 2576 loss: 28.942545
step: 2577 loss: 29.253532
step: 2578 loss: 28.52671
step: 2579 loss: 18.970177
step: 2580 loss: 25.910837
step: 2581 loss: 32.261345
step: 2582 loss: 29.07603
step: 2583 loss: 25.516703
step: 2584 loss: 25.627163
step: 2585 loss: 27.505684
step: 2586 loss: 25.47893
step: 2587 loss: 32.221222
step: 2588 loss: 32.594414
step: 2589 loss: 26.71502
step: 2590 loss: 25.191753
step: 2591 loss: 27.400784
step: 2592 loss: 26.883608
step: 2593 loss: 32.37342
step: 2594 loss: 31.705275
step: 2595 loss: 26.46896
step: 2596 loss: 27.03479
step: 2597 loss: 28.412521
step: 2598 loss: 31.260954
step: 2599 loss: 28.454014
step: 2600 loss: 22.389355
evaluating..
evaluation acc:  0.4335458558116724
evaluation acc(train):  0.7502453385672228
step: 2601 loss: 27.670765
step: 2602 loss: 30.214111
step: 2603 loss: 29.35979
step: 2604 loss: 27.764196
step: 2605 loss: 24.324297
step: 2606 loss: 26.692114
step: 2607 loss: 29.211384
step: 2608 loss: 26.512022
step: 2609 loss: 31.263504
step: 2610 loss: 28.967045
step: 2611 loss: 29.291655
step: 2612 loss: 28.190042
step: 2613 loss: 26.781715
step: 2614 loss: 28.997303
step: 2615 loss: 28.07066
step: 2616 loss: 27.692978
step: 2617 loss: 27.702545
step: 2618 loss: 29.649124
step: 2619 loss: 18.360226
step: 2620 loss: 23.85124
step: 2621 loss: 29.472023
step: 2622 loss: 26.386541
step: 2623 loss: 25.187553
step: 2624 loss: 28.604244
step: 2625 loss: 28.079868
step: 2626 loss: 25.93255
step: 2627 loss: 33.18827
step: 2628 loss: 30.985882
step: 2629 loss: 27.315853
step: 2630 loss: 23.432095
step: 2631 loss: 32.22749
step: 2632 loss: 27.28614
step: 2633 loss: 34.233494
step: 2634 loss: 32.51428
step: 2635 loss: 25.204891
step: 2636 loss: 26.222855
step: 2637 loss: 30.219027
step: 2638 loss: 32.020176
step: 2639 loss: 28.26253
step: 2640 loss: 20.538631
step: 2641 loss: 27.618824
step: 2642 loss: 32.0617
step: 2643 loss: 28.533318
step: 2644 loss: 25.951334
step: 2645 loss: 24.989208
step: 2646 loss: 25.14156
step: 2647 loss: 30.456715
step: 2648 loss: 27.430988
step: 2649 loss: 30.161953
step: 2650 loss: 27.462414
evaluating..
evaluation acc:  0.43795978420794507
evaluation acc(train):  0.751422963689892
step: 2651 loss: 29.918497
step: 2652 loss: 26.956274
step: 2653 loss: 27.807735
step: 2654 loss: 29.169151
step: 2655 loss: 28.569283
step: 2656 loss: 26.311722
step: 2657 loss: 30.167011
step: 2658 loss: 28.830017
step: 2659 loss: 19.398819
step: 2660 loss: 24.93
step: 2661 loss: 31.78209
step: 2662 loss: 29.356028
step: 2663 loss: 26.387049
step: 2664 loss: 26.942663
step: 2665 loss: 27.852036
step: 2666 loss: 25.681452
step: 2667 loss: 31.808296
step: 2668 loss: 33.118988
step: 2669 loss: 27.255404
step: 2670 loss: 24.385206
step: 2671 loss: 30.570883
step: 2672 loss: 26.235962
step: 2673 loss: 33.76642
step: 2674 loss: 33.750153
step: 2675 loss: 27.232754
step: 2676 loss: 25.422789
step: 2677 loss: 28.429558
step: 2678 loss: 30.817438
step: 2679 loss: 28.554459
step: 2680 loss: 21.03372
step: 2681 loss: 28.111004
step: 2682 loss: 30.411781
step: 2683 loss: 27.547075
step: 2684 loss: 26.115927
step: 2685 loss: 25.148897
step: 2686 loss: 25.572186
step: 2687 loss: 30.329086
step: 2688 loss: 25.286343
step: 2689 loss: 29.470484
step: 2690 loss: 28.107758
step: 2691 loss: 30.826546
step: 2692 loss: 25.128525
step: 2693 loss: 24.976337
step: 2694 loss: 28.206594
step: 2695 loss: 27.32076
step: 2696 loss: 28.814335
step: 2697 loss: 29.435823
step: 2698 loss: 29.343407
step: 2699 loss: 19.42
step: 2700 loss: 23.549973
evaluating..
evaluation acc:  0.43795978420794507
evaluation acc(train):  0.7525024533856722
step: 2701 loss: 30.551407
step: 2702 loss: 27.42368
step: 2703 loss: 25.686798
step: 2704 loss: 25.998081
step: 2705 loss: 28.11805
step: 2706 loss: 27.331888
step: 2707 loss: 31.112835
step: 2708 loss: 30.679916
step: 2709 loss: 27.604254
step: 2710 loss: 25.761923
step: 2711 loss: 29.522614
step: 2712 loss: 26.92076
step: 2713 loss: 33.31198
step: 2714 loss: 32.228798
step: 2715 loss: 27.936562
step: 2716 loss: 25.205633
step: 2717 loss: 27.968363
step: 2718 loss: 33.27131
step: 2719 loss: 28.711903
step: 2720 loss: 21.477104
step: 2721 loss: 26.471935
step: 2722 loss: 31.19432
step: 2723 loss: 26.945309
step: 2724 loss: 26.842426
step: 2725 loss: 24.369747
step: 2726 loss: 26.133959
step: 2727 loss: 31.191097
step: 2728 loss: 27.58773
step: 2729 loss: 28.41416
step: 2730 loss: 26.329962
step: 2731 loss: 32.220993
step: 2732 loss: 26.592962
step: 2733 loss: 26.957539
step: 2734 loss: 26.999538
step: 2735 loss: 27.281265
step: 2736 loss: 28.776752
step: 2737 loss: 27.098059
step: 2738 loss: 30.362326
step: 2739 loss: 19.925814
step: 2740 loss: 24.020496
step: 2741 loss: 30.38118
step: 2742 loss: 29.056648
step: 2743 loss: 26.153818
step: 2744 loss: 26.228601
step: 2745 loss: 28.556347
step: 2746 loss: 25.978891
step: 2747 loss: 29.93009
step: 2748 loss: 31.134129
step: 2749 loss: 26.949966
step: 2750 loss: 26.654396
evaluating..
evaluation acc:  0.4394310936733693
evaluation acc(train):  0.7538763493621198
step: 2751 loss: 28.784286
step: 2752 loss: 26.59106
step: 2753 loss: 32.08274
step: 2754 loss: 33.790718
step: 2755 loss: 27.317373
step: 2756 loss: 25.047132
step: 2757 loss: 27.797253
step: 2758 loss: 30.189268
step: 2759 loss: 28.053013
step: 2760 loss: 21.520279
step: 2761 loss: 28.429417
step: 2762 loss: 30.617456
step: 2763 loss: 27.80191
step: 2764 loss: 25.198105
step: 2765 loss: 24.928143
step: 2766 loss: 24.553673
step: 2767 loss: 31.252636
step: 2768 loss: 26.67036
step: 2769 loss: 32.259537
step: 2770 loss: 28.648941
step: 2771 loss: 32.121902
step: 2772 loss: 28.019848
step: 2773 loss: 25.80951
step: 2774 loss: 26.097609
step: 2775 loss: 27.85108
step: 2776 loss: 26.347683
step: 2777 loss: 27.384888
step: 2778 loss: 30.461723
step: 2779 loss: 18.28817
step: 2780 loss: 23.074097
step: 2781 loss: 30.872086
step: 2782 loss: 28.50716
step: 2783 loss: 27.545856
step: 2784 loss: 25.706573
step: 2785 loss: 26.178694
step: 2786 loss: 25.938408
step: 2787 loss: 30.07534
step: 2788 loss: 29.647905
step: 2789 loss: 25.871315
step: 2790 loss: 25.286612
step: 2791 loss: 30.200016
step: 2792 loss: 26.975075
step: 2793 loss: 33.428116
step: 2794 loss: 29.793478
step: 2795 loss: 27.846804
step: 2796 loss: 25.43161
step: 2797 loss: 28.981436
step: 2798 loss: 29.573013
step: 2799 loss: 27.986908
step: 2800 loss: 20.218657
evaluating..
evaluation acc:  0.43550760176557135
evaluation acc(train):  0.7558390578999019
step: 2801 loss: 28.421839
step: 2802 loss: 31.295958
step: 2803 loss: 29.609535
step: 2804 loss: 25.94494
step: 2805 loss: 24.315445
step: 2806 loss: 24.935484
step: 2807 loss: 29.579226
step: 2808 loss: 24.968235
step: 2809 loss: 30.893997
step: 2810 loss: 27.351929
step: 2811 loss: 31.798168
step: 2812 loss: 25.50913
step: 2813 loss: 25.207268
step: 2814 loss: 27.64636
step: 2815 loss: 28.103485
step: 2816 loss: 28.176306
step: 2817 loss: 27.494198
step: 2818 loss: 30.764725
step: 2819 loss: 19.24029
step: 2820 loss: 24.839695
step: 2821 loss: 31.751707
step: 2822 loss: 28.062065
step: 2823 loss: 26.174562
step: 2824 loss: 26.034288
step: 2825 loss: 27.003922
step: 2826 loss: 27.645912
step: 2827 loss: 32.06137
step: 2828 loss: 31.086311
step: 2829 loss: 26.24276
step: 2830 loss: 24.58998
step: 2831 loss: 31.195131
step: 2832 loss: 25.484655
step: 2833 loss: 30.166027
step: 2834 loss: 32.00081
step: 2835 loss: 24.708336
step: 2836 loss: 26.28586
step: 2837 loss: 27.95466
step: 2838 loss: 30.59944
step: 2839 loss: 27.763723
step: 2840 loss: 19.915638
step: 2841 loss: 26.600742
step: 2842 loss: 29.953278
step: 2843 loss: 27.43373
step: 2844 loss: 27.457558
step: 2845 loss: 23.747366
step: 2846 loss: 25.558975
step: 2847 loss: 30.12925
step: 2848 loss: 25.988379
step: 2849 loss: 30.351847
step: 2850 loss: 25.931725
evaluating..
evaluation acc:  0.4394310936733693
evaluation acc(train):  0.7581943081452405
step: 2851 loss: 29.930393
step: 2852 loss: 25.569233
step: 2853 loss: 26.046349
step: 2854 loss: 25.292736
step: 2855 loss: 29.298782
step: 2856 loss: 28.2933
step: 2857 loss: 27.810596
step: 2858 loss: 29.219296
step: 2859 loss: 18.94525
step: 2860 loss: 23.924023
step: 2861 loss: 31.16389
step: 2862 loss: 29.988186
step: 2863 loss: 25.626446
step: 2864 loss: 26.326494
step: 2865 loss: 28.158096
step: 2866 loss: 27.00468
step: 2867 loss: 31.21178
step: 2868 loss: 32.747196
step: 2869 loss: 27.851892
step: 2870 loss: 25.531897
step: 2871 loss: 29.897648
step: 2872 loss: 27.502295
step: 2873 loss: 32.161007
step: 2874 loss: 31.635466
step: 2875 loss: 26.46165
step: 2876 loss: 23.324446
step: 2877 loss: 27.873083
step: 2878 loss: 31.280262
step: 2879 loss: 30.903381
step: 2880 loss: 21.737886
step: 2881 loss: 26.643862
step: 2882 loss: 29.791698
step: 2883 loss: 23.519882
step: 2884 loss: 25.844646
step: 2885 loss: 25.26739
step: 2886 loss: 25.037693
step: 2887 loss: 30.686323
step: 2888 loss: 28.364437
step: 2889 loss: 29.452497
step: 2890 loss: 26.200714
step: 2891 loss: 27.769533
step: 2892 loss: 26.942719
step: 2893 loss: 26.587095
step: 2894 loss: 28.029263
step: 2895 loss: 30.787666
step: 2896 loss: 27.678013
step: 2897 loss: 27.328669
step: 2898 loss: 30.697802
step: 2899 loss: 19.37138
step: 2900 loss: 23.015915
evaluating..
evaluation acc:  0.4384502206964198
evaluation acc(train):  0.756918547595682
step: 2901 loss: 32.40357
step: 2902 loss: 27.940533
step: 2903 loss: 25.947458
step: 2904 loss: 26.977627
step: 2905 loss: 25.82575
step: 2906 loss: 25.79284
step: 2907 loss: 31.268627
step: 2908 loss: 30.43182
step: 2909 loss: 25.545532
step: 2910 loss: 26.318684
step: 2911 loss: 29.229218
step: 2912 loss: 25.791794
step: 2913 loss: 34.172768
step: 2914 loss: 32.87177
step: 2915 loss: 25.9487
step: 2916 loss: 23.600067
step: 2917 loss: 29.215883
step: 2918 loss: 30.432041
step: 2919 loss: 28.228226
step: 2920 loss: 21.816818
step: 2921 loss: 28.056606
step: 2922 loss: 28.751278
step: 2923 loss: 27.904484
step: 2924 loss: 27.709003
step: 2925 loss: 24.05238
step: 2926 loss: 24.582191
step: 2927 loss: 30.240255
step: 2928 loss: 26.074915
step: 2929 loss: 30.442757
step: 2930 loss: 26.545158
step: 2931 loss: 29.444433
step: 2932 loss: 27.44408
step: 2933 loss: 26.546436
step: 2934 loss: 26.997183
step: 2935 loss: 28.154068
step: 2936 loss: 29.383858
step: 2937 loss: 30.705254
step: 2938 loss: 28.27705
step: 2939 loss: 18.546883
step: 2940 loss: 24.416815
step: 2941 loss: 32.027145
step: 2942 loss: 27.975956
step: 2943 loss: 25.548616
step: 2944 loss: 25.25428
step: 2945 loss: 28.016436
step: 2946 loss: 24.374018
step: 2947 loss: 31.71408
step: 2948 loss: 29.439323
step: 2949 loss: 25.871784
step: 2950 loss: 23.002008
evaluating..
evaluation acc:  0.4394310936733693
evaluation acc(train):  0.7580961727183513
step: 2951 loss: 30.093601
step: 2952 loss: 25.751102
step: 2953 loss: 31.62545
step: 2954 loss: 32.668556
step: 2955 loss: 25.011639
step: 2956 loss: 24.33554
step: 2957 loss: 28.815025
step: 2958 loss: 30.11173
step: 2959 loss: 26.607025
step: 2960 loss: 19.88487
step: 2961 loss: 28.100065
step: 2962 loss: 28.11175
step: 2963 loss: 25.694592
step: 2964 loss: 25.37937
step: 2965 loss: 23.78076
step: 2966 loss: 26.386082
step: 2967 loss: 31.866234
step: 2968 loss: 26.282726
step: 2969 loss: 29.958406
step: 2970 loss: 26.12193
step: 2971 loss: 29.64231
step: 2972 loss: 27.056376
step: 2973 loss: 25.879313
step: 2974 loss: 25.585882
step: 2975 loss: 25.771898
step: 2976 loss: 26.348362
step: 2977 loss: 29.6343
step: 2978 loss: 29.286247
step: 2979 loss: 19.498253
step: 2980 loss: 24.111198
step: 2981 loss: 30.028637
step: 2982 loss: 29.342537
step: 2983 loss: 22.200184
step: 2984 loss: 25.583073
step: 2985 loss: 28.06799
step: 2986 loss: 26.20372
step: 2987 loss: 30.939121
step: 2988 loss: 31.625643
step: 2989 loss: 26.615965
step: 2990 loss: 25.510988
step: 2991 loss: 30.390268
step: 2992 loss: 24.653927
step: 2993 loss: 31.181717
step: 2994 loss: 30.672382
step: 2995 loss: 25.840742
step: 2996 loss: 25.734034
step: 2997 loss: 28.554901
step: 2998 loss: 30.792364
step: 2999 loss: 28.107311
step: 3000 loss: 20.680616
evaluating..
evaluation acc:  0.43795978420794507
evaluation acc(train):  0.7578999018645731
step: 3001 loss: 27.772114
step: 3002 loss: 29.877037
step: 3003 loss: 25.565697
step: 3004 loss: 26.591896
step: 3005 loss: 25.680428
step: 3006 loss: 24.644848
step: 3007 loss: 30.961657
step: 3008 loss: 23.526276
step: 3009 loss: 30.189754
step: 3010 loss: 25.95578
step: 3011 loss: 31.812065
step: 3012 loss: 27.625704
step: 3013 loss: 25.608847
step: 3014 loss: 26.803194
step: 3015 loss: 28.699423
step: 3016 loss: 27.543962
step: 3017 loss: 29.338585
step: 3018 loss: 29.024279
step: 3019 loss: 19.589981
step: 3020 loss: 23.506762
step: 3021 loss: 30.684378
step: 3022 loss: 27.41653
step: 3023 loss: 26.330109
step: 3024 loss: 25.725311
step: 3025 loss: 28.041504
step: 3026 loss: 25.175814
step: 3027 loss: 31.525188
step: 3028 loss: 29.996677
step: 3029 loss: 26.242077
step: 3030 loss: 24.936932
step: 3031 loss: 31.13366
step: 3032 loss: 27.025517
step: 3033 loss: 33.456383
step: 3034 loss: 30.993618
step: 3035 loss: 25.406588
step: 3036 loss: 24.668762
step: 3037 loss: 28.33503
step: 3038 loss: 28.72329
step: 3039 loss: 29.650967
step: 3040 loss: 22.12234
step: 3041 loss: 27.99024
step: 3042 loss: 29.075476
step: 3043 loss: 27.506882
step: 3044 loss: 24.604744
step: 3045 loss: 24.447538
step: 3046 loss: 24.705843
step: 3047 loss: 29.401016
step: 3048 loss: 26.372154
step: 3049 loss: 30.881184
step: 3050 loss: 27.062458
evaluating..
evaluation acc:  0.4369789112309956
evaluation acc(train):  0.7597644749754662
step: 3051 loss: 30.387592
step: 3052 loss: 24.63997
step: 3053 loss: 25.19873
step: 3054 loss: 27.993938
step: 3055 loss: 26.578983
step: 3056 loss: 27.197643
step: 3057 loss: 28.064116
step: 3058 loss: 28.72111
step: 3059 loss: 17.404293
step: 3060 loss: 25.009003
step: 3061 loss: 29.661964
step: 3062 loss: 29.02198
step: 3063 loss: 25.500278
step: 3064 loss: 23.403603
step: 3065 loss: 27.85718
step: 3066 loss: 24.183678
step: 3067 loss: 31.155338
step: 3068 loss: 29.449772
step: 3069 loss: 24.869709
step: 3070 loss: 24.923756
step: 3071 loss: 31.348637
step: 3072 loss: 26.8909
step: 3073 loss: 31.308182
step: 3074 loss: 31.68554
step: 3075 loss: 27.312756
step: 3076 loss: 24.365799
step: 3077 loss: 28.154022
step: 3078 loss: 29.67285
step: 3079 loss: 29.430317
step: 3080 loss: 21.6446
step: 3081 loss: 27.696997
step: 3082 loss: 27.518166
step: 3083 loss: 25.829206
step: 3084 loss: 26.211426
step: 3085 loss: 24.455166
step: 3086 loss: 25.345383
step: 3087 loss: 30.827469
step: 3088 loss: 25.180157
step: 3089 loss: 31.048916
step: 3090 loss: 26.505444
step: 3091 loss: 29.156624
step: 3092 loss: 26.671946
step: 3093 loss: 26.037056
step: 3094 loss: 27.087887
step: 3095 loss: 28.01186
step: 3096 loss: 26.431171
step: 3097 loss: 28.402056
step: 3098 loss: 29.37408
step: 3099 loss: 18.164824
step: 3100 loss: 23.709393
evaluating..
evaluation acc:  0.4384502206964198
evaluation acc(train):  0.7611383709519136
step: 3101 loss: 29.73633
step: 3102 loss: 28.475925
step: 3103 loss: 24.632322
step: 3104 loss: 25.345543
step: 3105 loss: 29.96389
step: 3106 loss: 25.402426
step: 3107 loss: 31.589817
step: 3108 loss: 29.061378
step: 3109 loss: 26.225842
step: 3110 loss: 23.172947
step: 3111 loss: 30.707325
step: 3112 loss: 26.947613
step: 3113 loss: 31.801956
step: 3114 loss: 31.217245
step: 3115 loss: 23.323612
step: 3116 loss: 25.631287
step: 3117 loss: 28.153008
step: 3118 loss: 30.50902
step: 3119 loss: 27.179853
step: 3120 loss: 19.416996
step: 3121 loss: 27.181515
step: 3122 loss: 29.640167
step: 3123 loss: 25.593468
step: 3124 loss: 25.693493
step: 3125 loss: 24.247799
step: 3126 loss: 26.110134
step: 3127 loss: 30.05177
step: 3128 loss: 24.976635
step: 3129 loss: 29.12082
step: 3130 loss: 26.990345
step: 3131 loss: 28.246365
step: 3132 loss: 27.11383
step: 3133 loss: 26.160934
step: 3134 loss: 27.535492
step: 3135 loss: 26.177135
step: 3136 loss: 26.88889
step: 3137 loss: 27.611124
step: 3138 loss: 29.423145
step: 3139 loss: 16.89252
step: 3140 loss: 23.954626
step: 3141 loss: 31.21286
step: 3142 loss: 28.124598
step: 3143 loss: 25.594635
step: 3144 loss: 25.131676
step: 3145 loss: 26.896278
step: 3146 loss: 25.715607
step: 3147 loss: 30.760765
step: 3148 loss: 31.195639
step: 3149 loss: 27.230478
step: 3150 loss: 23.962837
evaluating..
evaluation acc:  0.4394310936733693
evaluation acc(train):  0.7606476938174681
step: 3151 loss: 28.554985
step: 3152 loss: 26.234188
step: 3153 loss: 29.778868
step: 3154 loss: 34.38272
step: 3155 loss: 24.599533
step: 3156 loss: 25.155197
step: 3157 loss: 27.688559
step: 3158 loss: 30.739439
step: 3159 loss: 27.478956
step: 3160 loss: 20.133245
step: 3161 loss: 25.608528
step: 3162 loss: 29.668564
step: 3163 loss: 27.409113
step: 3164 loss: 26.18703
step: 3165 loss: 23.512058
step: 3166 loss: 24.213943
step: 3167 loss: 28.114489
step: 3168 loss: 23.342224
step: 3169 loss: 30.293877
step: 3170 loss: 27.141722
step: 3171 loss: 29.184486
step: 3172 loss: 25.342983
step: 3173 loss: 25.550854
step: 3174 loss: 26.266205
step: 3175 loss: 28.155428
step: 3176 loss: 27.219856
step: 3177 loss: 28.011623
step: 3178 loss: 29.342747
step: 3179 loss: 16.264297
step: 3180 loss: 25.925735
step: 3181 loss: 28.216415
step: 3182 loss: 27.660538
step: 3183 loss: 23.45952
step: 3184 loss: 24.029099
step: 3185 loss: 27.23178
step: 3186 loss: 24.724525
step: 3187 loss: 28.99685
step: 3188 loss: 30.859144
step: 3189 loss: 29.622852
step: 3190 loss: 23.559834
step: 3191 loss: 30.187778
step: 3192 loss: 27.344368
step: 3193 loss: 31.656809
step: 3194 loss: 31.73926
step: 3195 loss: 25.803135
step: 3196 loss: 22.936218
step: 3197 loss: 28.642431
step: 3198 loss: 30.108055
step: 3199 loss: 28.174664
step: 3200 loss: 21.35332
evaluating..
evaluation acc:  0.43894065718489456
evaluation acc(train):  0.7618253189401374
evaluating..
evaluation acc:  0.43894065718489456
evaluation acc(train):  0.7618253189401374
